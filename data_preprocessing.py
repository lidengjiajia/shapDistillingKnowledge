"""
æ•°æ®é¢„å¤„ç†æ¨¡å— - æ›´æ–°ç‰ˆ
Data Preprocessing Module - Updated Version
é€‚é…æ–°çš„ç¥ç»ç½‘ç»œè®­ç»ƒæ¡†æ¶
"""

import torch
import torch.nn as nn
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡ç°æ€§
torch.manual_seed(42)
np.random.seed(42)
if torch.cuda.is_available():
    torch.cuda.manual_seed(42)
    torch.cuda.manual_seed_all(42)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
import random
random.seed(42)

# è®¾å¤‡é…ç½®
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸ”§ Using device: {device}")

class DataPreprocessor:
    """æ•°æ®é¢„å¤„ç†å™¨ç±»"""
    
    def __init__(self):
        self.scalers = {}
        self.label_encoders = {}
        self.feature_names = {}
    
    def load_german_credit(self):
        """åŠ è½½Germanä¿¡ç”¨æ•°æ®é›†"""
        print("ğŸ”„ Loading German Credit dataset...")
        
        # ä»æœ¬åœ°CSVæ–‡ä»¶åŠ è½½
        try:
            df = pd.read_csv('data/german_credit.csv')
        except FileNotFoundError:
            print("âŒ German credit data file not found. Please ensure 'data/german_credit.csv' exists.")
            return None
        
        print(f"German Credit original shape: {df.shape}")
        
        # æ£€æŸ¥ç›®æ ‡åˆ—
        if 'class' in df.columns:
            # å°†ç›®æ ‡å˜é‡è½¬æ¢ä¸ºäºŒè¿›åˆ¶ï¼ˆ1=è‰¯å¥½ä¿¡ç”¨ï¼Œ0=ä¸è‰¯ä¿¡ç”¨ï¼‰
            df['class'] = df['class'].replace({1: 1, 2: 0})  # 1=good, 2=bad -> 1=good, 0=bad
            target_col = 'class'
        elif 'Class' in df.columns:
            df['Class'] = df['Class'].replace({1: 1, 2: 0})
            target_col = 'Class'
        else:
            # å‡è®¾æœ€åä¸€åˆ—æ˜¯ç›®æ ‡åˆ—
            target_col = df.columns[-1]
            df[target_col] = df[target_col].replace({1: 1, 2: 0})
        
        # è¯†åˆ«åˆ†ç±»å˜é‡å’Œæ•°å€¼å˜é‡
        categorical_cols = []
        numerical_cols = []
        
        for col in df.columns:
            if col != target_col:
                if df[col].dtype == 'object' or df[col].nunique() <= 10:
                    categorical_cols.append(col)
                else:
                    numerical_cols.append(col)
        
        # å¤„ç†åˆ†ç±»å˜é‡ - One-hotç¼–ç 
        if categorical_cols:
            df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)
        
        # åˆ†å‰²ç‰¹å¾å’Œç›®æ ‡
        X = df.drop(target_col, axis=1)
        y = df[target_col]
        
        print(f"German Credit processed shape: {X.shape}")
        print(f"Features: {X.shape[1]}, Samples: {X.shape[0]}")
        
        # åˆ†å‰²æ•°æ®é›†ï¼š60% è®­ç»ƒï¼Œ20% éªŒè¯ï¼Œ20% æµ‹è¯•
        X_temp, X_test, y_temp, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        X_train, X_val, y_train, y_val = train_test_split(
            X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp  # 0.25 x 0.8 = 0.2 of total
        )
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_val_scaled = scaler.transform(X_val)
        X_test_scaled = scaler.transform(X_test)
        
        # ä¿å­˜scalerå’Œç‰¹å¾å
        self.scalers['german'] = scaler
        self.feature_names['german'] = list(X.columns)
        
        print(f"German dataset split: Train: {X_train_scaled.shape}, Val: {X_val_scaled.shape}, Test: {X_test_scaled.shape}")
        
        return X_train_scaled, X_val_scaled, X_test_scaled, y_train.values, y_val.values, y_test.values
    
    def load_australian_credit(self):
        """åŠ è½½Australianä¿¡ç”¨æ•°æ®é›†"""
        print("ğŸ”„ Loading Australian Credit dataset...")
        
        try:
            df = pd.read_csv('data/australian_credit.csv')
        except FileNotFoundError:
            print("âŒ Australian credit data file not found. Please ensure 'data/australian_credit.csv' exists.")
            return None
        
        print(f"Australian Credit original shape: {df.shape}")
        
        # æ£€æŸ¥ç›®æ ‡åˆ—
        if 'Class' in df.columns:
            target_col = 'Class'
        elif 'class' in df.columns:
            target_col = 'class'
        else:
            # å‡è®¾æœ€åä¸€åˆ—æ˜¯ç›®æ ‡åˆ—
            target_col = df.columns[-1]
        
        # ç¡®ä¿ç›®æ ‡å˜é‡æ˜¯0å’Œ1
        unique_values = df[target_col].unique()
        if len(unique_values) == 2:
            # å°†ç›®æ ‡å˜é‡æ˜ å°„ä¸º0å’Œ1
            value_mapping = {unique_values[0]: 0, unique_values[1]: 1}
            df[target_col] = df[target_col].map(value_mapping)
        
        # å¤„ç†ç¼ºå¤±å€¼
        for col in df.columns:
            if col != target_col:
                if df[col].dtype == 'object':
                    df[col].fillna(df[col].mode()[0], inplace=True)
                else:
                    df[col].fillna(df[col].median(), inplace=True)
        
        # è¯†åˆ«å¹¶å¤„ç†åˆ†ç±»å˜é‡
        categorical_cols = []
        for col in df.columns:
            if col != target_col and (df[col].dtype == 'object' or df[col].nunique() <= 10):
                categorical_cols.append(col)
        
        # One-hotç¼–ç 
        if categorical_cols:
            df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)
        
        # åˆ†å‰²ç‰¹å¾å’Œç›®æ ‡
        X = df.drop(target_col, axis=1)
        y = df[target_col]
        
        print(f"Australian Credit processed shape: {X.shape}")
        print(f"Features: {X.shape[1]}, Samples: {X.shape[0]}")
        
        # åˆ†å‰²æ•°æ®é›†ï¼š60% è®­ç»ƒï¼Œ20% éªŒè¯ï¼Œ20% æµ‹è¯•
        X_temp, X_test, y_temp, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        X_train, X_val, y_train, y_val = train_test_split(
            X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp
        )
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_val_scaled = scaler.transform(X_val)
        X_test_scaled = scaler.transform(X_test)
        
        # ä¿å­˜scalerå’Œç‰¹å¾å
        self.scalers['australian'] = scaler
        self.feature_names['australian'] = list(X.columns)
        
        print(f"Australian dataset split: Train: {X_train_scaled.shape}, Val: {X_val_scaled.shape}, Test: {X_test_scaled.shape}")
        
        return X_train_scaled, X_val_scaled, X_test_scaled, y_train.values, y_val.values, y_test.values
    
    def load_uci_credit(self):
        """åŠ è½½UCIä¿¡ç”¨æ•°æ®é›†"""
        print("ğŸ”„ Loading UCI Credit dataset...")
        
        try:
            # å°è¯•è¯»å–Excelæ–‡ä»¶
            df = pd.read_excel('data/uci_credit.xls', header=1, index_col=0)
        except FileNotFoundError:
            print("âŒ UCI credit data file not found. Please ensure 'data/uci_credit.xls' exists.")
            return None
        except Exception as e:
            print(f"âŒ Error loading UCI credit data: {e}")
            return None
        
        print(f"UCI Credit original shape: {df.shape}")
        
        # é‡å‘½åç›®æ ‡åˆ—
        if 'default payment next month' in df.columns:
            df.rename(columns={'default payment next month': 'DEFAULT'}, inplace=True)
            target_col = 'DEFAULT'
        elif 'DEFAULT' in df.columns:
            target_col = 'DEFAULT'
        else:
            # å‡è®¾æœ€åä¸€åˆ—æ˜¯ç›®æ ‡åˆ—
            target_col = df.columns[-1]
        
        # å¤„ç†å¼‚å¸¸å€¼å’Œç¼ºå¤±å€¼
        # ç§»é™¤IDåˆ—å¦‚æœå­˜åœ¨
        if 'ID' in df.columns:
            df.drop('ID', axis=1, inplace=True)
        
        # å¤„ç†æ€§åˆ«ç¼–ç å¼‚å¸¸å€¼
        if 'SEX' in df.columns:
            df['SEX'] = df['SEX'].replace({0: 2})  # 0æ›¿æ¢ä¸º2ï¼Œç¡®ä¿åªæœ‰1å’Œ2
        
        # å¤„ç†æ•™è‚²å’Œå©šå§»çŠ¶å†µçš„å¼‚å¸¸å€¼
        if 'EDUCATION' in df.columns:
            df['EDUCATION'] = df['EDUCATION'].replace({0: 4, 5: 4, 6: 4})  # åˆå¹¶æœªçŸ¥ç±»åˆ«
        
        if 'MARRIAGE' in df.columns:
            df['MARRIAGE'] = df['MARRIAGE'].replace({0: 3})  # 0æ›¿æ¢ä¸º3
        
        # åˆ†å‰²ç‰¹å¾å’Œç›®æ ‡
        X = df.drop(target_col, axis=1)
        y = df[target_col]
        
        print(f"UCI Credit processed shape: {X.shape}")
        print(f"Features: {X.shape[1]}, Samples: {X.shape[0]}")
        
        # åˆ†å‰²æ•°æ®é›†ï¼š60% è®­ç»ƒï¼Œ20% éªŒè¯ï¼Œ20% æµ‹è¯•
        X_temp, X_test, y_temp, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        X_train, X_val, y_train, y_val = train_test_split(
            X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp
        )
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_val_scaled = scaler.transform(X_val)
        X_test_scaled = scaler.transform(X_test)
        
        # ä¿å­˜scalerå’Œç‰¹å¾å
        self.scalers['uci'] = scaler
        self.feature_names['uci'] = list(X.columns)
        
        print(f"UCI dataset split: Train: {X_train_scaled.shape}, Val: {X_val_scaled.shape}, Test: {X_test_scaled.shape}")
        
        return X_train_scaled, X_val_scaled, X_test_scaled, y_train.values, y_val.values, y_test.values
    
    def split_and_scale_data(self, X, y, feature_names, test_size=0.2, val_size=0.2):
        """åˆ†å‰²å’Œæ ‡å‡†åŒ–æ•°æ®"""
        # é¦–å…ˆåˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        X_train_val, X_test, y_train_val, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=y
        )
        
        # å†ä»è®­ç»ƒé›†ä¸­åˆ†å‰²å‡ºéªŒè¯é›†
        val_size_adjusted = val_size / (1 - test_size)  # è°ƒæ•´éªŒè¯é›†æ¯”ä¾‹
        X_train, X_val, y_train, y_val = train_test_split(
            X_train_val, y_train_val, test_size=val_size_adjusted, random_state=42, stratify=y_train_val
        )
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        scaler = StandardScaler()
        X_train = scaler.fit_transform(X_train)
        X_val = scaler.transform(X_val)
        X_test = scaler.transform(X_test)
        
        return {
            'X_train': X_train,
            'X_val': X_val,
            'X_test': X_test,
            'y_train': y_train,
            'y_val': y_val,
            'y_test': y_test,
            'feature_names': feature_names,
            'scaler': scaler
        }
    
    def process_all_datasets(self):
        """å¤„ç†æ‰€æœ‰æ•°æ®é›†"""
        print("ğŸ“Š Processing all datasets...")
        
        processed_data = {}
        
        # å¤„ç†Germanæ•°æ®é›†
        german_data = self.load_german_credit()
        if german_data is not None:
            X_train, X_val, X_test, y_train, y_val, y_test = german_data
            processed_data['german'] = {
                'X_train': X_train, 'X_val': X_val, 'X_test': X_test,
                'y_train': y_train, 'y_val': y_val, 'y_test': y_test,
                'feature_names': self.feature_names['german'],
                'scaler': self.scalers['german']
            }
            print(f"ğŸ”§ Processing german dataset...")
            print(f"german dataset split:")
            print(f"  Train: {processed_data['german']['X_train'].shape}, Val: {processed_data['german']['X_val'].shape}, Test: {processed_data['german']['X_test'].shape}")
            print(f"  Class distribution - Train: {np.bincount(processed_data['german']['y_train'])}, Val: {np.bincount(processed_data['german']['y_val'])}, Test: {np.bincount(processed_data['german']['y_test'])}")
        
        # å¤„ç†Australianæ•°æ®é›†
        australian_data = self.load_australian_credit()
        if australian_data is not None:
            X_train, X_val, X_test, y_train, y_val, y_test = australian_data
            processed_data['australian'] = {
                'X_train': X_train, 'X_val': X_val, 'X_test': X_test,
                'y_train': y_train, 'y_val': y_val, 'y_test': y_test,
                'feature_names': self.feature_names['australian'],
                'scaler': self.scalers['australian']
            }
            print(f"ğŸ”§ Processing australian dataset...")
            print(f"australian dataset split:")
            print(f"  Train: {processed_data['australian']['X_train'].shape}, Val: {processed_data['australian']['X_val'].shape}, Test: {processed_data['australian']['X_test'].shape}")
            print(f"  Class distribution - Train: {np.bincount(processed_data['australian']['y_train'])}, Val: {np.bincount(processed_data['australian']['y_val'])}, Test: {np.bincount(processed_data['australian']['y_test'])}")
        
        # å¤„ç†UCIæ•°æ®é›†
        uci_data = self.load_uci_credit()
        if uci_data is not None:
            X_train, X_val, X_test, y_train, y_val, y_test = uci_data
            processed_data['uci'] = {
                'X_train': X_train, 'X_val': X_val, 'X_test': X_test,
                'y_train': y_train, 'y_val': y_val, 'y_test': y_test,
                'feature_names': self.feature_names['uci'],
                'scaler': self.scalers['uci']
            }
            print(f"ğŸ”§ Processing uci dataset...")
            print(f"uci dataset split:")
            print(f"  Train: {processed_data['uci']['X_train'].shape}, Val: {processed_data['uci']['X_val'].shape}, Test: {processed_data['uci']['X_test'].shape}")
            print(f"  Class distribution - Train: {np.bincount(processed_data['uci']['y_train'])}, Val: {np.bincount(processed_data['uci']['y_val'])}, Test: {np.bincount(processed_data['uci']['y_test'])}")
        
        print("âœ… All datasets processed successfully!")
        return processed_data

if __name__ == "__main__":
    # æµ‹è¯•æ•°æ®é¢„å¤„ç†
    preprocessor = DataPreprocessor()
    processed_data = preprocessor.process_all_datasets()
    
    for dataset_name, data in processed_data.items():
        print(f"\n{dataset_name.upper()} Dataset Summary:")
        print(f"  Features: {len(data['feature_names'])}")
        print(f"  Train samples: {data['X_train'].shape[0]}")
        print(f"  Validation samples: {data['X_val'].shape[0]}")
        print(f"  Test samples: {data['X_test'].shape[0]}")
        print(f"  Class distribution (train): {np.bincount(data['y_train'])}")
