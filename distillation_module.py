# Knowledge Distillation Module - Decision Tree Only
# çŸ¥è¯†è’¸é¦æ¨¡å— - ä»…å†³ç­–æ ‘ç‰ˆæœ¬

import numpy as np
import torch
import torch.nn.functional as F
from sklearn.tree import DecisionTreeClassifier, _tree
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import classification_report
from sklearn.model_selection import GridSearchCV
from tqdm import tqdm
import warnings
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import partial
import os
from datetime import datetime
import pandas as pd

# å¹¶å‘é…ç½®ï¼šä½¿ç”¨CPUæ ¸å¿ƒæ•°-1ï¼Œè‡³å°‘ä¸º1
n_jobs = max(1, min(os.cpu_count() - 1, os.cpu_count()))
# åªåœ¨éœ€è¦æ—¶æ˜¾ç¤ºé…ç½®ä¿¡æ¯ï¼Œé¿å…é‡å¤è¾“å‡º

# å¯¼å…¥æ¶ˆèå®éªŒåˆ†æå™¨
from ablation_analyzer import ablation_analyzer

# åˆ›å»ºTop-kæ¶ˆèåˆ†æå™¨çš„å…¨å±€å®ä¾‹
topk_ablation_analyzer = None

warnings.filterwarnings('ignore')

# è®¾ç½®matplotlibåç«¯ä¸ºéäº¤äº’å¼ï¼Œé¿å…å¤šçº¿ç¨‹é—®é¢˜
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt

# ç¦ç”¨Optunaæ—¥å¿—è¾“å‡º

class KnowledgeDistillator:
    """çŸ¥è¯†è’¸é¦ç³»ç»Ÿ - å†³ç­–æ ‘è’¸é¦"""
    
    def __init__(self, teacher_models, processed_data, all_shap_results):
        self.teacher_models = teacher_models
        self.processed_data = processed_data
        self.all_shap_results = all_shap_results
        
    def extract_knowledge(self, dataset_name, model_type, temperature=3.0):
        """ä»æ•™å¸ˆæ¨¡å‹æå–çŸ¥è¯†
        
        çŸ¥è¯†è’¸é¦ç†è®ºèƒŒæ™¯ï¼š
        æ•™å¸ˆæ¨¡å‹è¾“å‡ºsoftmaxåˆ†å¸ƒåŒ…å«æ›´ä¸°å¯Œçš„ç±»é—´å…³ç³»ä¿¡æ¯
        temperatureå‚æ•°æ§åˆ¶åˆ†å¸ƒçš„å¹³æ»‘ç¨‹åº¦ï¼Œæ¸©åº¦è¶Šé«˜åˆ†å¸ƒè¶Šå¹³æ»‘
        """
        teacher_model = self.teacher_models[dataset_name]['model']
        data_dict = self.processed_data[dataset_name]
        
        X_train = data_dict['X_train']
        X_test = data_dict['X_test']
        
        # è·å–æ•™å¸ˆæ¨¡å‹çš„è½¯æ ‡ç­¾(æ¦‚ç‡åˆ†å¸ƒ)
        train_logits = self._get_teacher_predictions(teacher_model, X_train)
        test_logits = self._get_teacher_predictions(teacher_model, X_test)
        
        # åº”ç”¨æ¸©åº¦ç¼©æ”¾ï¼Œå¢å¼ºçŸ¥è¯†è’¸é¦æ•ˆæœ
        train_soft_labels = self._apply_temperature(train_logits, temperature)
        test_soft_labels = self._apply_temperature(test_logits, temperature)
        
        return {
            'train_soft_labels': train_soft_labels,
            'test_soft_labels': test_soft_labels,
            'teacher_logits_train': train_logits,
            'teacher_logits_test': test_logits
        }
    
    def _get_teacher_predictions(self, teacher_model, X):
        """ä»æ•™å¸ˆæ¨¡å‹è·å–é¢„æµ‹æ¦‚ç‡ - å…¼å®¹PyTorchå’Œsklearnæ¨¡å‹"""
        import torch
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯PyTorchæ¨¡å‹
        if hasattr(teacher_model, 'eval') and hasattr(teacher_model, 'forward'):
            # PyTorchæ¨¡å‹
            teacher_model.eval()
            device = next(teacher_model.parameters()).device
            
            with torch.no_grad():
                X_tensor = torch.FloatTensor(X).to(device)
                outputs = teacher_model(X_tensor)
                # å¯¹äºäºŒåˆ†ç±»ï¼Œå°†sigmoidè¾“å‡ºè½¬æ¢ä¸ºä¸¤ç±»æ¦‚ç‡
                probs_class1 = outputs.cpu().numpy().flatten()
                probs_class0 = 1 - probs_class1
                return np.column_stack([probs_class0, probs_class1])
        else:
            # sklearnæ¨¡å‹
            return teacher_model.predict_proba(X)
        
        return {
            'train_soft_labels': train_soft_labels,
            'test_soft_labels': test_soft_labels,
            'teacher_logits_train': train_logits,
            'teacher_logits_test': test_logits
        }
    
    def _apply_temperature(self, logits, temperature):
        """æ¸©åº¦ç¼©æ”¾ï¼šlogits / Tï¼Œç„¶ååº”ç”¨softmax
        æ¸©åº¦T > 1 ä½¿åˆ†å¸ƒæ›´å¹³æ»‘ï¼ŒT < 1 ä½¿åˆ†å¸ƒæ›´sharp
        """
        return F.softmax(torch.tensor(logits) / temperature, dim=1).numpy()
    
    def train_student_model(self, dataset_name, model_type_name='decision_tree', 
                          k=5, temperature=3.0, alpha=0.7, max_depth=6, 
                          use_all_features=False, trial=None):
        """è®­ç»ƒå­¦ç”Ÿæ¨¡å‹(å†³ç­–æ ‘)ä½¿ç”¨çŸ¥è¯†è’¸é¦
        
        å‚æ•°:
        - dataset_name: æ•°æ®é›†åç§°
        - model_type_name: å­¦ç”Ÿæ¨¡å‹ç±»å‹ï¼Œå›ºå®šä¸º'decision_tree'
        - k: Top-kç‰¹å¾æ•°é‡
        - temperature: çŸ¥è¯†è’¸é¦æ¸©åº¦å‚æ•°
        - alpha: è’¸é¦æŸå¤±æƒé‡ (0=ä»…ç¡¬æ ‡ç­¾, 1=ä»…è½¯æ ‡ç­¾)
        - max_depth: å†³ç­–æ ‘æœ€å¤§æ·±åº¦
        - use_all_features: æ˜¯å¦ä½¿ç”¨å…¨ç‰¹å¾
        - trial: Optuna trialå¯¹è±¡(ç”¨äºè¶…å‚æ•°ä¼˜åŒ–)
        """
        
        data_dict = self.processed_data[dataset_name]
        
        # ç‰¹å¾é€‰æ‹©
        if use_all_features:
            # ä½¿ç”¨å…¨ç‰¹å¾
            X_train_selected = data_dict['X_train']
            X_test_selected = data_dict['X_test']
            selected_features = data_dict['feature_names']
            model_type = f'all_features_decision_tree_distillation'
        else:
            # é€‰æ‹©Top-kç‰¹å¾
            shap_results = self.all_shap_results[dataset_name]
            top_k_features = shap_results['top_k_features'][k]
            feature_indices = [data_dict['feature_names'].index(feat) for feat in top_k_features]
            
            X_train_selected = data_dict['X_train'][:, feature_indices]
            X_test_selected = data_dict['X_test'][:, feature_indices]
            selected_features = top_k_features
            model_type = f'top_{k}_decision_tree_distillation'
        
        y_train = data_dict['y_train']
        y_test = data_dict['y_test']
        
        # æå–æ•™å¸ˆæ¨¡å‹çŸ¥è¯†
        knowledge = self.extract_knowledge(dataset_name, 'teacher', temperature)
        train_soft_labels = knowledge['train_soft_labels']
        test_soft_labels = knowledge['test_soft_labels']
        
        # åˆ›å»ºå†³ç­–æ ‘å­¦ç”Ÿæ¨¡å‹
        student_model = self._create_decision_tree_student(trial, max_depth)
        
        # çŸ¥è¯†è’¸é¦è®­ç»ƒ
        student_model = self._train_with_distillation(
            student_model, X_train_selected, y_train, train_soft_labels, alpha
        )
        
        # é¢„æµ‹å’Œè¯„ä¼°
        y_pred = student_model.predict(X_test_selected)
        y_pred_proba = student_model.predict_proba(X_test_selected)
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
        recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
        f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)
        
        # æå–å†³ç­–è§„åˆ™
        rules = self._extract_decision_rules(student_model, selected_features)
        
        return {
            'model': student_model,
            'model_type': model_type,
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'feature_count': len(selected_features),
            'selected_features': selected_features,
            'predictions': y_pred,
            'probabilities': y_pred_proba,
            'rules': rules,
            'temperature': temperature,
            'alpha': alpha,
            'max_depth': max_depth,
            'hyperparameters': {
                'temperature': temperature,
                'alpha': alpha,
                'max_depth': max_depth
            }
        }
    
    def _create_decision_tree_student(self, trial, max_depth):
        """åˆ›å»ºå†³ç­–æ ‘å­¦ç”Ÿæ¨¡å‹"""
        if trial is not None:
            # Optunaè¶…å‚æ•°ä¼˜åŒ–
            trial_max_depth = trial.suggest_int('max_depth', 3, 12)
            min_samples_split = trial.suggest_int('min_samples_split', 2, 20)
            min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)
            max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2', None])
        else:
            # ä½¿ç”¨å›ºå®šå‚æ•°
            trial_max_depth = max_depth
            min_samples_split = 2
            min_samples_leaf = 1
            max_features = 'sqrt'
        
        return DecisionTreeClassifier(
            max_depth=trial_max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            max_features=max_features,
            random_state=42
        )
    
    def _train_with_distillation(self, model, X_train, y_train, soft_labels, alpha):
        """ä½¿ç”¨çŸ¥è¯†è’¸é¦è®­ç»ƒå†³ç­–æ ‘
        
        å¯¹äºå†³ç­–æ ‘ï¼Œæˆ‘ä»¬ä½¿ç”¨è½¯æ ‡ç­¾çš„æ¦‚ç‡ä½œä¸ºæ ·æœ¬æƒé‡
        è¿™æ˜¯ä¸€ç§è¿‘ä¼¼çš„çŸ¥è¯†è’¸é¦æ–¹æ³•ï¼Œå› ä¸ºå†³ç­–æ ‘ä¸ç›´æ¥æ”¯æŒè½¯æ ‡ç­¾
        """
        
        if alpha > 0:
            # ä½¿ç”¨è½¯æ ‡ç­¾çš„æœ€å¤§æ¦‚ç‡ä½œä¸ºæ ·æœ¬æƒé‡
            sample_weights = np.max(soft_labels, axis=1)
            # å½’ä¸€åŒ–æƒé‡
            sample_weights = sample_weights / np.sum(sample_weights) * len(sample_weights)
            
            # è®­ç»ƒæ—¶ä½¿ç”¨æ ·æœ¬æƒé‡
            model.fit(X_train, y_train, sample_weight=sample_weights)
        else:
            # çº¯ç¡¬æ ‡ç­¾è®­ç»ƒ
            model.fit(X_train, y_train)
        
        return model
    
    def _extract_decision_rules(self, model, feature_names):
        """æå–å†³ç­–æ ‘è§„åˆ™"""
        # ç®€åŒ–è§„åˆ™æå–ï¼Œä¸ä¾èµ–å¤–éƒ¨æ¨¡å—
        rules = self._simple_extract_rules(model, feature_names)
        
        return {
            'rules': rules,
            'rule_count': len(rules),
            'description': f'Decision tree with {len(rules)} rules'
        }
    
    def _simple_extract_rules(self, model, feature_names):
        """ç®€å•çš„å†³ç­–æ ‘è§„åˆ™æå–"""
        tree = model.tree_
        rules = []
        
        def recurse(node, depth, parent_rule=""):
            if tree.feature[node] != _tree.TREE_UNDEFINED:
                name = feature_names[tree.feature[node]]
                threshold = tree.threshold[node]
                left_rule = f"{parent_rule}{name} <= {threshold:.4f}"
                right_rule = f"{parent_rule}{name} > {threshold:.4f}"
                recurse(tree.children_left[node], depth + 1, left_rule + " and ")
                recurse(tree.children_right[node], depth + 1, right_rule + " and ")
            else:
                # å¶å­èŠ‚ç‚¹
                if parent_rule:
                    rule = parent_rule.rstrip(" and ")
                    value = tree.value[node]
                    predicted_class = np.argmax(value)
                    confidence = np.max(value) / np.sum(value)
                    rules.append(f"IF {rule} THEN class={predicted_class} (confidence={confidence:.4f})")
        
        try:
            recurse(0, 0)
        except Exception as e:
            # å¦‚æœè§„åˆ™æå–å¤±è´¥ï¼Œè¿”å›ç®€å•æè¿°
            rules = [f"Decision tree with {tree.node_count} nodes"]
        
        return rules
    
    def train_baseline_decision_tree(self, dataset_name, max_depth=5):
        """è®­ç»ƒåŸºç¡€å†³ç­–æ ‘ï¼ˆä¸ä½¿ç”¨è’¸é¦ï¼‰
        è¿™æ˜¯4ç§æ¨¡å‹å¯¹æ¯”ä¸­çš„ç¬¬1ç§ï¼šåŸå§‹å†³ç­–æ ‘
        """
        data_dict = self.processed_data[dataset_name]
        
        X_train = data_dict['X_train']
        X_test = data_dict['X_test']
        y_train = data_dict['y_train']
        y_test = data_dict['y_test']
        feature_names = data_dict['feature_names']
        
        # å›ºå®šå‚æ•°è®­ç»ƒåŸºç¡€å†³ç­–æ ‘ï¼ˆæ— Optunaï¼‰
        model = DecisionTreeClassifier(
            max_depth=max_depth,
            min_samples_split=2,
            min_samples_leaf=1,
            max_features='sqrt',
            random_state=42
        )
        model.fit(X_train, y_train)

        # é¢„æµ‹å’Œè¯„ä¼°
        y_pred = model.predict(X_test)
        y_pred_proba = model.predict_proba(X_test)

        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
        recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
        f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)

        # æå–å†³ç­–è§„åˆ™
        rules = self._extract_decision_rules(model, feature_names)

        params = {
            'max_depth': 5,
            'min_samples_split': 2,
            'min_samples_leaf': 1,
            'max_features': 'sqrt'
        }

        return {
            'model': model,
            'model_type': 'baseline_tree',
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'feature_count': len(feature_names),
            'selected_features': feature_names,
            'predictions': y_pred,
            'probabilities': y_pred_proba,
            'rules': rules,
            'hyperparameters': params,
            'best_params': params
        }
    
    def run_all_feature_distillation(self, dataset_names, temperature_range, alpha_range, max_depth_range):
        """è¿è¡Œå…¨ç‰¹å¾çŸ¥è¯†è’¸é¦å®éªŒå¹¶è®°å½•æ¶ˆèå®éªŒæ•°æ®"""
        results = {}
        
        for dataset_name in dataset_names:
            print(f"   Processing {dataset_name.upper()} dataset...")
            results[dataset_name] = {}
            
            best_accuracy = 0  # æ”¹ä¸ºä½¿ç”¨å‡†ç¡®ç‡ä½œä¸ºè¯„åˆ¤æ ‡å‡†
            best_result = None
            
            total_combinations = len(temperature_range) * len(alpha_range) * len(max_depth_range)
            progress_bar = tqdm(total=total_combinations, desc=f"ğŸ“ {dataset_name.upper()}", 
                               unit="exp", position=0, leave=True)
            print(f"     å…¨ç‰¹å¾å®éªŒç»„åˆæ•°: {total_combinations}")
            
            for temperature in temperature_range:
                for alpha in alpha_range:
                    for max_depth in max_depth_range:
                        progress_bar.set_postfix({
                            'T': temperature, 
                            'Î±': f"{alpha:.1f}", 
                            'D': max_depth,
                            'Best': f"{best_accuracy:.4f}"
                        })
                        result = self.train_student_model(
                            dataset_name=dataset_name,
                            model_type_name='decision_tree',
                            use_all_features=True,
                            temperature=temperature,
                            alpha=alpha,
                            max_depth=max_depth
                        )
                        
                        # è®°å½•å…¨ç‰¹å¾è’¸é¦çš„æ¶ˆèå®éªŒæ•°æ®
                        ablation_analyzer.record_experiment_result(
                            dataset_name=dataset_name,
                            k=None,  # å…¨ç‰¹å¾è’¸é¦æ²¡æœ‰kå€¼
                            temperature=temperature,
                            alpha=alpha,
                            max_depth=max_depth,
                            accuracy=result['accuracy'],
                            f1_score=result['f1'],
                            precision=result['precision'],
                            recall=result['recall']
                        )
                        
                        if result['accuracy'] > best_accuracy:  # æ”¹ä¸ºä½¿ç”¨å‡†ç¡®ç‡
                            best_accuracy = result['accuracy']
                            best_result = result
                        
                        progress_bar.update(1)
            
            progress_bar.close()
            results[dataset_name]['best'] = best_result
            print(f"     Best Accuracy: {best_accuracy:.4f}")  # æ”¹ä¸ºæ˜¾ç¤ºå‡†ç¡®ç‡
        
        # ä¿å­˜æ¶ˆèå®éªŒæ•°æ®å’Œåˆ›å»ºå¯è§†åŒ–
        print("\nğŸ“Š Saving ablation study data and creating visualizations for all-feature distillation...")
        ablation_analyzer.save_ablation_data(prefix='ablation_study')
        ablation_analyzer.create_ablation_visualizations()
        ablation_analyzer.generate_summary_report(prefix='ablation_study')
        
        return results
    
    
    def run_comprehensive_distillation(self, dataset_names, k_ranges, temperature_range, alpha_range, max_depth_range):
        """è¿è¡Œç»¼åˆçŸ¥è¯†è’¸é¦å®éªŒï¼ˆTop-kç‰¹å¾ï¼‰
        
        Args:
            dataset_names: æ•°æ®é›†åç§°åˆ—è¡¨
            k_ranges: æ¯ä¸ªæ•°æ®é›†çš„kèŒƒå›´å­—å…¸ {'german': (5, 54), 'australian': (5, 22), 'uci': (5, 23)}
            temperature_range: æ¸©åº¦å‚æ•°èŒƒå›´
            alpha_range: åŠ æƒå‚æ•°èŒƒå›´
            max_depth_range: å†³ç­–æ ‘æ·±åº¦èŒƒå›´
        """
        global topk_ablation_analyzer
        
        # åˆå§‹åŒ–Top-kæ¶ˆèåˆ†æå™¨
        from ablation_analyzer import AblationStudyAnalyzer
        topk_ablation_analyzer = AblationStudyAnalyzer()
        topk_ablation_analyzer.experiment_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        results = {}
        
        for dataset_name in dataset_names:
            print(f"   Processing {dataset_name.upper()} dataset...")
            results[dataset_name] = {}
            
            best_accuracy = 0  # æ”¹ä¸ºä½¿ç”¨å‡†ç¡®ç‡ä½œä¸ºè¯„åˆ¤æ ‡å‡†
            best_result = None
            best_k = None
            
            # è·å–å½“å‰æ•°æ®é›†çš„kèŒƒå›´
            dataset_k_range = k_ranges[dataset_name]
            k_values = list(range(dataset_k_range[0], dataset_k_range[1] + 1))
            total_combinations = len(k_values) * len(temperature_range) * len(alpha_range) * len(max_depth_range)
            progress_bar = tqdm(total=total_combinations, desc=f"ğŸ” {dataset_name.upper()}", 
                               unit="exp", position=0, leave=True)
            print(f"     kèŒƒå›´: {dataset_k_range[0]} åˆ° {dataset_k_range[1]} ({len(k_values)} ä¸ªå€¼)")
            print(f"     Top-kå®éªŒç»„åˆæ•°: {total_combinations}")
            
            # å‡†å¤‡å¹¶å‘æ‰§è¡Œçš„å®éªŒå‚æ•°
            experiment_params = []
            for k in k_values:
                for temperature in temperature_range:
                    for alpha in alpha_range:
                        for max_depth in max_depth_range:
                            experiment_params.append((dataset_name, k, temperature, alpha, max_depth))
            
            # è®¾ç½®å¹¶å‘æ•°é‡ï¼ˆä½¿ç”¨çº¿ç¨‹æ± é¿å…multiprocessingçš„ç¼–ç é—®é¢˜ï¼‰
            import platform
            if platform.system() == 'Windows':
                n_jobs = min(4, max(1, os.cpu_count() // 2))
            else:
                n_jobs = max(1, min(os.cpu_count() - 1, os.cpu_count()))
            
            print(f"     ğŸš€ Using {n_jobs} parallel threads for Top-k distillation")
            
            # å¹¶å‘æ‰§è¡Œå®éªŒ
            def run_single_experiment(params):
                dataset_name, k, temperature, alpha, max_depth = params
                try:
                    result = self.train_student_model(
                        dataset_name=dataset_name,
                        model_type_name='decision_tree',
                        k=k,
                        temperature=temperature,
                        alpha=alpha,
                        max_depth=max_depth,
                        use_all_features=False
                    )
                    return params, result, None
                except Exception as e:
                    return params, None, str(e)
            
            # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæ‰§è¡Œï¼ˆé¿å…multiprocessingçš„ç¼–ç é—®é¢˜ï¼‰
            all_results = []
            with ThreadPoolExecutor(max_workers=n_jobs) as executor:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                future_to_params = {
                    executor.submit(run_single_experiment, params): params 
                    for params in experiment_params
                }
                
                # å¤„ç†ç»“æœ
                for future in as_completed(future_to_params):
                    params, result, error = future.result()
                    if error:
                        print(f"     âŒ Error in experiment {params}: {error}")
                        continue
                    
                    dataset_name, k, temperature, alpha, max_depth = params
                    
                    # è®°å½•Top-kè’¸é¦çš„æ¶ˆèå®éªŒæ•°æ®
                    topk_ablation_analyzer.record_experiment_result(
                        dataset_name=dataset_name,
                        k=k,
                        temperature=temperature,
                        alpha=alpha,
                        max_depth=max_depth,
                        accuracy=result['accuracy'],
                        f1_score=result['f1'],
                        precision=result['precision'],
                        recall=result['recall']
                    )
                    
                    all_results.append((params, result))
                    
                    if result['accuracy'] > best_accuracy:
                        best_accuracy = result['accuracy']
                        best_result = result
                        best_k = k
                    
                    # æ›´æ–°è¿›åº¦æ¡
                    progress_bar.set_postfix({
                        'k': k,
                        'T': temperature, 
                        'Î±': f"{alpha:.1f}", 
                        'D': max_depth,
                        'Best': f"{best_accuracy:.4f}"
                    })
                    progress_bar.update(1)
            
            progress_bar.close()
            results[dataset_name]['best'] = best_result
            results[dataset_name]['best_k'] = best_k
            print(f"     Best Accuracy: {best_accuracy:.4f} with k={best_k}")  # æ”¹ä¸ºæ˜¾ç¤ºå‡†ç¡®ç‡
            
            # ğŸŒ² ä¿å­˜æœ€ä½³SHAP-KDæ¨¡å‹çš„å†³ç­–æ ‘è§„åˆ™å’Œè·¯å¾„
            if best_result is not None:
                print(f"\n   ğŸŒ² Saving best SHAP-KD model rules and paths for {dataset_name.upper()}...")
                data_dict = self.processed_data[dataset_name]
                
                # ä¿å­˜å†³ç­–è§„åˆ™
                best_params = {
                    'k': best_k,
                    'temperature': best_result['temperature'],
                    'alpha': best_result['alpha'],
                    'max_depth': best_result['max_depth']
                }
                self._save_decision_tree_rules(
                    model=best_result['model'],
                    dataset_name=dataset_name,
                    model_type='SHAP-KD',
                    feature_names=best_result['selected_features'],
                    params=best_params
                )
                
                # ä¿å­˜å†³ç­–è·¯å¾„
                selected_feature_names = best_result['selected_features']
                feature_indices = [data_dict['feature_names'].index(feat) for feat in selected_feature_names]
                X_test_selected = data_dict['X_test'][:, feature_indices]
                
                self._save_decision_tree_paths(
                    model=best_result['model'],
                    dataset_name=dataset_name,
                    model_type='SHAP-KD',
                    X_test=X_test_selected,
                    y_test=data_dict['y_test'],
                    feature_names=selected_feature_names,
                    params=best_params
                )
        
        # ä¿å­˜Top-kæ¶ˆèå®éªŒæ•°æ®å’Œåˆ›å»ºå¯è§†åŒ–
        print("\nğŸ“Š Saving Top-k ablation study data and creating visualizations...")
        topk_ablation_analyzer.save_ablation_data(prefix='topk_ablation_study')
        # ä½¿ç”¨é€šç”¨çš„æ¶ˆèå®éªŒå¯è§†åŒ–æ–¹æ³•ï¼ˆé¿å…é‡å¤ç”Ÿæˆï¼‰
        topk_ablation_analyzer.create_ablation_visualizations()
        topk_ablation_analyzer.generate_summary_report(prefix='topk_ablation_study')
        
        return results
    
    def run_four_model_comparison(self, dataset_names, best_params):
        """è¿è¡Œ4ç§æ¨¡å‹å¯¹æ¯”å®éªŒ
        
        4ç§æ¨¡å‹ï¼š
        1. Baseline Decision Tree - åŸå§‹å†³ç­–æ ‘ï¼ˆä¸ä½¿ç”¨è’¸é¦ï¼‰
        2. Teacher Model - ç¥ç»ç½‘ç»œæ•™å¸ˆæ¨¡å‹
        3. FKD - Full Knowledge Distillationï¼ˆä½¿ç”¨å…¨éƒ¨ç‰¹å¾çš„çŸ¥è¯†è’¸é¦ï¼‰
        4. SHAP-KD - Top-k Knowledge Distillationï¼ˆä½¿ç”¨SHAPé€‰æ‹©çš„Top-kç‰¹å¾ï¼‰
        
        Args:
            dataset_names: æ•°æ®é›†åç§°åˆ—è¡¨
            best_params: æ¯ä¸ªæ•°æ®é›†çš„æœ€ä½³å‚æ•°å­—å…¸
                æ ¼å¼: {'german': {'k': 10, 'temperature': 3, 'alpha': 0.5, 'max_depth': 5}, ...}
        
        Returns:
            comparison_results: åŒ…å«4ç§æ¨¡å‹å¯¹æ¯”ç»“æœçš„å­—å…¸
        """
        comparison_results = {}
        
        for dataset_name in dataset_names:
            print(f"\nğŸ”¬ Running 4-Model Comparison for {dataset_name.upper()}...")
            comparison_results[dataset_name] = {}
            
            data_dict = self.processed_data[dataset_name]
            X_test = data_dict['X_test']
            y_test = data_dict['y_test']
            
            # è·å–è¯¥æ•°æ®é›†çš„æœ€ä½³å‚æ•°
            params = best_params.get(dataset_name, {
                'k': 10, 'temperature': 3.0, 'alpha': 0.5, 'max_depth': 5
            })
            
            # 1ï¸âƒ£ åŸå§‹å†³ç­–æ ‘ (Baseline Decision Tree)
            print(f"   1ï¸âƒ£ Training Baseline Decision Tree...")
            baseline_dt_result = self.train_baseline_decision_tree(
                dataset_name, 
                max_depth=params.get('max_depth', 5)
            )
            comparison_results[dataset_name]['baseline_dt'] = {
                'model_name': 'Baseline Decision Tree',
                'accuracy': baseline_dt_result['accuracy'],
                'precision': baseline_dt_result['precision'],
                'recall': baseline_dt_result['recall'],
                'f1': baseline_dt_result['f1'],
                'feature_count': baseline_dt_result['feature_count'],
                'max_depth': params.get('max_depth', 5)
            }
            print(f"      Accuracy: {baseline_dt_result['accuracy']:.4f}")
            
            # 2ï¸âƒ£ æ•™å¸ˆæ¨¡å‹ (Teacher Neural Network)
            print(f"   2ï¸âƒ£ Evaluating Teacher Model...")
            teacher_model = self.teacher_models[dataset_name]['model']
            teacher_pred = self._get_teacher_hard_predictions(teacher_model, X_test)
            teacher_accuracy = accuracy_score(y_test, teacher_pred)
            teacher_precision = precision_score(y_test, teacher_pred, average='weighted', zero_division=0)
            teacher_recall = recall_score(y_test, teacher_pred, average='weighted', zero_division=0)
            teacher_f1 = f1_score(y_test, teacher_pred, average='weighted', zero_division=0)
            
            comparison_results[dataset_name]['teacher'] = {
                'model_name': 'Teacher Neural Network',
                'accuracy': teacher_accuracy,
                'precision': teacher_precision,
                'recall': teacher_recall,
                'f1': teacher_f1,
                'feature_count': len(data_dict['feature_names'])
            }
            print(f"      Accuracy: {teacher_accuracy:.4f}")
            
            # 3ï¸âƒ£ å…¨ç‰¹å¾çŸ¥è¯†è’¸é¦ (Full Knowledge Distillation - FKD)
            print(f"   3ï¸âƒ£ Training FKD (Full Knowledge Distillation)...")
            fkd_result = self.train_student_model(
                dataset_name=dataset_name,
                model_type_name='decision_tree',
                k=None,  # ä¸ä½¿ç”¨k
                temperature=params.get('temperature', 3.0),
                alpha=params.get('alpha', 0.5),
                max_depth=params.get('max_depth', 5),
                use_all_features=True  # ä½¿ç”¨å…¨éƒ¨ç‰¹å¾
            )
            comparison_results[dataset_name]['fkd'] = {
                'model_name': 'FKD (All Features)',
                'accuracy': fkd_result['accuracy'],
                'precision': fkd_result['precision'],
                'recall': fkd_result['recall'],
                'f1': fkd_result['f1'],
                'feature_count': fkd_result['feature_count'],
                'temperature': params.get('temperature', 3.0),
                'alpha': params.get('alpha', 0.5),
                'max_depth': params.get('max_depth', 5)
            }
            print(f"      Accuracy: {fkd_result['accuracy']:.4f}")
            
            # 4ï¸âƒ£ Top-kçŸ¥è¯†è’¸é¦ (SHAP-KD)
            print(f"   4ï¸âƒ£ Training SHAP-KD (Top-{params.get('k', 10)} Features)...")
            shap_kd_result = self.train_student_model(
                dataset_name=dataset_name,
                model_type_name='decision_tree',
                k=params.get('k', 10),
                temperature=params.get('temperature', 3.0),
                alpha=params.get('alpha', 0.5),
                max_depth=params.get('max_depth', 5),
                use_all_features=False  # ä½¿ç”¨Top-kç‰¹å¾
            )
            comparison_results[dataset_name]['shap_kd'] = {
                'model_name': f'SHAP-KD (Top-{params.get("k", 10)})',
                'accuracy': shap_kd_result['accuracy'],
                'precision': shap_kd_result['precision'],
                'recall': shap_kd_result['recall'],
                'f1': shap_kd_result['f1'],
                'feature_count': shap_kd_result['feature_count'],
                'k': params.get('k', 10),
                'temperature': params.get('temperature', 3.0),
                'alpha': params.get('alpha', 0.5),
                'max_depth': params.get('max_depth', 5)
            }
            print(f"      Accuracy: {shap_kd_result['accuracy']:.4f}")
            
            # æ³¨æ„ï¼šè§„åˆ™å’Œè·¯å¾„æå–å·²åœ¨æ¶ˆèå®éªŒçš„æœ€ä½³æ¨¡å‹ä¸­å®Œæˆï¼Œæ­¤å¤„ä¸é‡å¤ä¿å­˜
            # å¦‚éœ€å•ç‹¬ä¿å­˜å››æ¨¡å‹å¯¹æ¯”çš„è§„åˆ™ï¼Œå¯å–æ¶ˆä¸‹æ–¹æ³¨é‡Š
            # # ğŸŒ² ä¿å­˜SHAP-KDå†³ç­–æ ‘è§„åˆ™
            # print(f"   ğŸŒ² Extracting and saving SHAP-KD decision tree rules...")
            # self._save_decision_tree_rules(
            #     model=shap_kd_result['model'],
            #     dataset_name=dataset_name,
            #     model_type='SHAP-KD',
            #     feature_names=shap_kd_result.get('feature_names', data_dict['feature_names'][:params.get('k', 10)]),
            #     params=params
            # )
            # 
            # # ğŸ›¤ï¸ ä¿å­˜SHAP-KDå†³ç­–æ ‘è·¯å¾„ï¼ˆæ¯ä¸ªæ ·æœ¬çš„å…·ä½“è·¯å¾„ï¼‰
            # print(f"   ğŸ›¤ï¸ Extracting and saving SHAP-KD decision tree paths...")
            # 
            # # è·å–SHAP-KDä½¿ç”¨çš„ç‰¹å¾ç´¢å¼•
            # selected_feature_names = shap_kd_result.get('selected_features', data_dict['feature_names'][:params.get('k', 10)])
            # if isinstance(selected_feature_names, list) and isinstance(selected_feature_names[0], str):
            #     # å¦‚æœæ˜¯ç‰¹å¾åç§°åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºç´¢å¼•
            #     feature_indices = [data_dict['feature_names'].index(feat) for feat in selected_feature_names]
            #     X_test_selected = data_dict['X_test'][:, feature_indices]
            # else:
            #     # å¦‚æœå·²ç»æ˜¯ç´¢å¼•ï¼Œç›´æ¥ä½¿ç”¨
            #     X_test_selected = data_dict['X_test'][:, :params.get('k', 10)]
            #     selected_feature_names = data_dict['feature_names'][:params.get('k', 10)]
            # 
            # self._save_decision_tree_paths(
            #     model=shap_kd_result['model'],
            #     dataset_name=dataset_name,
            #     model_type='SHAP-KD',
            #     X_test=X_test_selected,
            #     y_test=data_dict['y_test'],
            #     feature_names=selected_feature_names,
            #     params=params
            # )
            
            print(f"\n   âœ… {dataset_name.upper()} Comparison Complete")
            print(f"      Baseline DT: {baseline_dt_result['accuracy']:.4f}")
            print(f"      Teacher: {teacher_accuracy:.4f}")
            print(f"      FKD: {fkd_result['accuracy']:.4f}")
            print(f"      SHAP-KD: {shap_kd_result['accuracy']:.4f}")
        
        return comparison_results
    
    def _get_teacher_hard_predictions(self, teacher_model, X):
        """è·å–æ•™å¸ˆæ¨¡å‹çš„ç¡¬é¢„æµ‹ï¼ˆç±»åˆ«æ ‡ç­¾ï¼‰"""
        import torch
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯PyTorchæ¨¡å‹
        if hasattr(teacher_model, 'eval') and hasattr(teacher_model, 'forward'):
            # PyTorchæ¨¡å‹
            teacher_model.eval()
            device = next(teacher_model.parameters()).device
            
            with torch.no_grad():
                X_tensor = torch.FloatTensor(X).to(device)
                outputs = teacher_model(X_tensor)
                # å¯¹äºäºŒåˆ†ç±»ï¼Œå°†sigmoidè¾“å‡ºè½¬æ¢ä¸ºç±»åˆ«
                probs = outputs.cpu().numpy().flatten()
                predictions = (probs > 0.5).astype(int)
                return predictions
        else:
            # sklearnæ¨¡å‹
            return teacher_model.predict(X)
    
    def save_four_model_comparison_to_excel(self, comparison_results, timestamp):
        """ä¿å­˜4ç§æ¨¡å‹å¯¹æ¯”ç»“æœåˆ°Excel
        
        Args:
            comparison_results: run_four_model_comparisonè¿”å›çš„ç»“æœå­—å…¸
            timestamp: æ—¶é—´æˆ³å­—ç¬¦ä¸²
        
        Returns:
            filename: ä¿å­˜çš„Excelæ–‡ä»¶è·¯å¾„
        """
        filename = f"results/four_model_comparison_{timestamp}.xlsx"
        
        with pd.ExcelWriter(filename, engine='openpyxl') as writer:
            # ä¸ºæ¯ä¸ªæ•°æ®é›†åˆ›å»ºä¸€ä¸ªsheet
            for dataset_name, models in comparison_results.items():
                data = []
                for model_key, model_result in models.items():
                    row = {
                        'Model': model_result['model_name'],
                        'Accuracy': f"{model_result['accuracy']:.4f}",
                        'Precision': f"{model_result['precision']:.4f}",
                        'Recall': f"{model_result['recall']:.4f}",
                        'F1_Score': f"{model_result['f1']:.4f}",
                        'Feature_Count': model_result.get('feature_count', 'N/A')
                    }
                    
                    # æ·»åŠ ç‰¹å®šæ¨¡å‹çš„é¢å¤–å‚æ•°
                    if 'k' in model_result:
                        row['k'] = model_result['k']
                    if 'temperature' in model_result:
                        row['Temperature'] = model_result['temperature']
                    if 'alpha' in model_result:
                        row['Alpha'] = model_result['alpha']
                    if 'max_depth' in model_result:
                        row['Max_Depth'] = model_result['max_depth']
                    
                    data.append(row)
                
                df = pd.DataFrame(data)
                df.to_excel(writer, sheet_name=dataset_name.upper(), index=False)
            
            # åˆ›å»ºæ±‡æ€»sheet
            summary_data = []
            for dataset_name, models in comparison_results.items():
                for model_key, model_result in models.items():
                    summary_data.append({
                        'Dataset': dataset_name.upper(),
                        'Model': model_result['model_name'],
                        'Accuracy': f"{model_result['accuracy']:.4f}",
                        'F1_Score': f"{model_result['f1']:.4f}",
                        'Feature_Count': model_result.get('feature_count', 'N/A')
                    })
            
            summary_df = pd.DataFrame(summary_data)
            # æŒ‰æ•°æ®é›†å’Œå‡†ç¡®ç‡æ’åº
            summary_df = summary_df.sort_values(['Dataset', 'Accuracy'], ascending=[True, False])
            summary_df.to_excel(writer, sheet_name='SUMMARY', index=False)
        
        print(f"\nğŸ“Š Four-Model Comparison saved to: {filename}")
        return filename
    
    def _save_decision_tree_rules(self, model, dataset_name, model_type, feature_names, params):
        """æå–å¹¶ä¿å­˜å†³ç­–æ ‘è§„åˆ™åˆ°æ–‡æœ¬æ–‡ä»¶
        
        Args:
            model: è®­ç»ƒå¥½çš„å†³ç­–æ ‘æ¨¡å‹
            dataset_name: æ•°æ®é›†åç§°
            model_type: æ¨¡å‹ç±»å‹ (å¦‚ 'SHAP-KD', 'FKD', 'Baseline')
            feature_names: ç‰¹å¾åç§°åˆ—è¡¨
            params: æ¨¡å‹å‚æ•°å­—å…¸
        """
        from sklearn.tree import _tree
        import os
        
        # åˆ›å»ºresultsç›®å½•
        os.makedirs('results', exist_ok=True)
        
        # æ„å»ºæ–‡ä»¶å
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"results/{dataset_name}_{model_type}_decision_rules_{timestamp}.txt"
        
        tree = model.tree_
        
        with open(filename, 'w', encoding='utf-8') as f:
            # å†™å…¥å¤´éƒ¨ä¿¡æ¯
            f.write("="*80 + "\n")
            f.write(f"å†³ç­–æ ‘è§„åˆ™æå– - {dataset_name.upper()} æ•°æ®é›†\n")
            f.write(f"æ¨¡å‹ç±»å‹: {model_type}\n")
            f.write("="*80 + "\n\n")
            
            # å†™å…¥æ¨¡å‹å‚æ•°
            f.write("æ¨¡å‹å‚æ•°:\n")
            f.write("-"*80 + "\n")
            for key, value in params.items():
                f.write(f"  {key}: {value}\n")
            f.write("\n")
            
            # å†™å…¥æ ‘çš„åŸºæœ¬ä¿¡æ¯
            f.write("å†³ç­–æ ‘ä¿¡æ¯:\n")
            f.write("-"*80 + "\n")
            f.write(f"  èŠ‚ç‚¹æ€»æ•°: {tree.node_count}\n")
            f.write(f"  æœ€å¤§æ·±åº¦: {tree.max_depth}\n")
            f.write(f"  å¶å­èŠ‚ç‚¹æ•°: {tree.n_leaves}\n")
            f.write(f"  ä½¿ç”¨ç‰¹å¾æ•°: {len(feature_names)}\n")
            f.write("\n")
            
            # å†™å…¥ç‰¹å¾åç§°
            f.write("ä½¿ç”¨çš„ç‰¹å¾:\n")
            f.write("-"*80 + "\n")
            for i, fname in enumerate(feature_names, 1):
                f.write(f"  {i}. {fname}\n")
            f.write("\n")
            
            # æå–å¹¶å†™å…¥å†³ç­–è§„åˆ™
            f.write("å†³ç­–è§„åˆ™:\n")
            f.write("="*80 + "\n\n")
            
            rule_count = [0]  # ä½¿ç”¨åˆ—è¡¨ä»¥ä¾¿åœ¨é€’å½’å‡½æ•°ä¸­ä¿®æ”¹
            
            def recurse(node, depth, rule_path=""):
                """é€’å½’æå–è§„åˆ™"""
                indent = "  " * depth
                
                if tree.feature[node] != _tree.TREE_UNDEFINED:
                    # å†…éƒ¨èŠ‚ç‚¹
                    feature_name = feature_names[tree.feature[node]]
                    threshold = tree.threshold[node]
                    
                    # å·¦å­æ ‘ (<=)
                    left_rule = f"{rule_path}{'AND ' if rule_path else ''}({feature_name} <= {threshold:.4f})"
                    recurse(tree.children_left[node], depth + 1, left_rule + " ")
                    
                    # å³å­æ ‘ (>)
                    right_rule = f"{rule_path}{'AND ' if rule_path else ''}({feature_name} > {threshold:.4f})"
                    recurse(tree.children_right[node], depth + 1, right_rule + " ")
                else:
                    # å¶å­èŠ‚ç‚¹ - ç”Ÿæˆè§„åˆ™
                    rule_count[0] += 1
                    value = tree.value[node][0]
                    predicted_class = np.argmax(value)
                    samples = np.sum(value)
                    confidence = np.max(value) / samples if samples > 0 else 0
                    
                    f.write(f"è§„åˆ™ {rule_count[0]}:\n")
                    f.write(f"{indent}IF {rule_path.strip()}\n")
                    f.write(f"{indent}THEN é¢„æµ‹ç±»åˆ« = {predicted_class}\n")
                    f.write(f"{indent}     ç½®ä¿¡åº¦ = {confidence:.4f} ({int(np.max(value))}/{int(samples)} æ ·æœ¬)\n")
                    f.write(f"{indent}     æ ·æœ¬åˆ†å¸ƒ = {[int(x) for x in value]}\n")
                    f.write("\n")
            
            # å¼€å§‹é€’å½’æå–
            try:
                recurse(0, 0)
                f.write("="*80 + "\n")
                f.write(f"æ€»å…±æå–äº† {rule_count[0]} æ¡è§„åˆ™\n")
                f.write("="*80 + "\n")
            except Exception as e:
                f.write(f"\næå–è§„åˆ™æ—¶å‡ºé”™: {str(e)}\n")
        
        print(f"      Rules saved to: {filename}")
        return filename
    
    def _save_decision_tree_paths(self, model, dataset_name, model_type, X_test, y_test, feature_names, params):
        """æå–å¹¶ä¿å­˜æ¯ä¸ªæµ‹è¯•æ ·æœ¬é€šè¿‡å†³ç­–æ ‘çš„è·¯å¾„
        
        Args:
            model: è®­ç»ƒå¥½çš„å†³ç­–æ ‘æ¨¡å‹
            dataset_name: æ•°æ®é›†åç§°
            model_type: æ¨¡å‹ç±»å‹ (å¦‚ 'SHAP-KD')
            X_test: æµ‹è¯•é›†ç‰¹å¾
            y_test: æµ‹è¯•é›†çœŸå®æ ‡ç­¾
            feature_names: ç‰¹å¾åç§°åˆ—è¡¨
            params: æ¨¡å‹å‚æ•°å­—å…¸
        """
        from sklearn.tree import _tree
        import os
        
        # åˆ›å»ºresultsç›®å½•
        os.makedirs('results', exist_ok=True)
        
        # æ„å»ºæ–‡ä»¶å
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"results/{dataset_name}_{model_type}_tree_paths_{timestamp}.txt"
        
        tree = model.tree_
        y_pred = model.predict(X_test)
        
        # è·å–æ¯ä¸ªæ ·æœ¬çš„å†³ç­–è·¯å¾„
        decision_paths = model.decision_path(X_test)
        
        with open(filename, 'w', encoding='utf-8') as f:
            # å†™å…¥å¤´éƒ¨ä¿¡æ¯
            f.write("="*100 + "\n")
            f.write(f"å†³ç­–æ ‘è·¯å¾„æå– - {dataset_name.upper()} æ•°æ®é›†\n")
            f.write(f"æ¨¡å‹ç±»å‹: {model_type}\n")
            f.write("="*100 + "\n\n")
            
            # å†™å…¥æ¨¡å‹å‚æ•°
            f.write("æ¨¡å‹å‚æ•°:\n")
            f.write("-"*100 + "\n")
            for key, value in params.items():
                f.write(f"  {key}: {value}\n")
            f.write("\n")
            
            # å†™å…¥ç»Ÿè®¡ä¿¡æ¯
            f.write("è·¯å¾„ç»Ÿè®¡:\n")
            f.write("-"*100 + "\n")
            f.write(f"  æµ‹è¯•æ ·æœ¬æ€»æ•°: {len(X_test)}\n")
            f.write(f"  ç‰¹å¾æ•°é‡: {len(feature_names)}\n")
            f.write(f"  æ ‘çš„æœ€å¤§æ·±åº¦: {tree.max_depth}\n")
            f.write(f"  é¢„æµ‹å‡†ç¡®ç‡: {accuracy_score(y_test, y_pred):.4f}\n")
            f.write("\n")
            
            # æå–æ¯ä¸ªæ ·æœ¬çš„è·¯å¾„
            f.write("æ ·æœ¬å†³ç­–è·¯å¾„:\n")
            f.write("="*100 + "\n\n")
            
            # è®¡ç®—è·¯å¾„é•¿åº¦åˆ†å¸ƒ
            path_lengths = []
            
            for sample_idx in range(len(X_test)):
                # è·å–è¯¥æ ·æœ¬ç»è¿‡çš„èŠ‚ç‚¹
                node_indices = decision_paths.indices[decision_paths.indptr[sample_idx]:
                                                     decision_paths.indptr[sample_idx + 1]]
                
                path_length = len(node_indices)
                path_lengths.append(path_length)
                
                # å†™å…¥æ ·æœ¬ä¿¡æ¯
                f.write(f"æ ·æœ¬ {sample_idx + 1}:\n")
                f.write(f"  çœŸå®æ ‡ç­¾: {int(y_test[sample_idx])}, é¢„æµ‹æ ‡ç­¾: {int(y_pred[sample_idx])}, ")
                f.write(f"é¢„æµ‹ç»“æœ: {'âœ“æ­£ç¡®' if y_test[sample_idx] == y_pred[sample_idx] else 'âœ—é”™è¯¯'}\n")
                f.write(f"  è·¯å¾„é•¿åº¦: {path_length} (ç»è¿‡ {path_length} ä¸ªèŠ‚ç‚¹)\n")
                f.write(f"  å†³ç­–è·¯å¾„:\n")
                
                # é€ä¸ªèŠ‚ç‚¹è¿½è¸ªè·¯å¾„
                for depth, node_id in enumerate(node_indices):
                    indent = "    " + "  " * depth
                    
                    if tree.feature[node_id] != _tree.TREE_UNDEFINED:
                        # å†…éƒ¨èŠ‚ç‚¹
                        feature_name = feature_names[tree.feature[node_id]]
                        threshold = tree.threshold[node_id]
                        feature_value = X_test[sample_idx, tree.feature[node_id]]
                        
                        # åˆ¤æ–­èµ°å‘
                        if feature_value <= threshold:
                            direction = "å·¦åˆ†æ”¯ (â‰¤)"
                            symbol = "âœ“"
                        else:
                            direction = "å³åˆ†æ”¯ (>)"
                            symbol = "âœ—"
                        
                        f.write(f"{indent}èŠ‚ç‚¹ {node_id}: [{feature_name}] = {feature_value:.4f} ")
                        f.write(f"{symbol} (é˜ˆå€¼: {threshold:.4f}) â†’ {direction}\n")
                    else:
                        # å¶å­èŠ‚ç‚¹
                        value = tree.value[node_id][0]
                        predicted_class = np.argmax(value)
                        samples = np.sum(value)
                        confidence = np.max(value) / samples if samples > 0 else 0
                        
                        f.write(f"{indent}å¶å­èŠ‚ç‚¹ {node_id}: é¢„æµ‹ç±»åˆ« = {predicted_class}, ")
                        f.write(f"ç½®ä¿¡åº¦ = {confidence:.4f}, æ ·æœ¬åˆ†å¸ƒ = {[int(x) for x in value]}\n")
                
                f.write("\n")
                
                # æ¯50ä¸ªæ ·æœ¬è¾“å‡ºä¸€æ¬¡è¿›åº¦
                if (sample_idx + 1) % 50 == 0:
                    f.write(f"--- å·²å¤„ç† {sample_idx + 1}/{len(X_test)} ä¸ªæ ·æœ¬ ---\n\n")
            
            # å†™å…¥è·¯å¾„ç»Ÿè®¡åˆ†æ
            f.write("="*100 + "\n")
            f.write("è·¯å¾„é•¿åº¦åˆ†æ:\n")
            f.write("-"*100 + "\n")
            f.write(f"  å¹³å‡è·¯å¾„é•¿åº¦: {np.mean(path_lengths):.2f}\n")
            f.write(f"  æœ€çŸ­è·¯å¾„: {np.min(path_lengths)}\n")
            f.write(f"  æœ€é•¿è·¯å¾„: {np.max(path_lengths)}\n")
            f.write(f"  è·¯å¾„é•¿åº¦æ ‡å‡†å·®: {np.std(path_lengths):.2f}\n")
            f.write("\n")
            
            # æŒ‰è·¯å¾„é•¿åº¦ç»Ÿè®¡æ ·æœ¬æ•°
            unique_lengths, counts = np.unique(path_lengths, return_counts=True)
            f.write("è·¯å¾„é•¿åº¦åˆ†å¸ƒ:\n")
            for length, count in zip(unique_lengths, counts):
                percentage = count / len(path_lengths) * 100
                f.write(f"  é•¿åº¦ {length}: {count} ä¸ªæ ·æœ¬ ({percentage:.2f}%)\n")
            
            f.write("="*100 + "\n")
            f.write(f"æ€»å…±æå–äº† {len(X_test)} ä¸ªæ ·æœ¬çš„å†³ç­–è·¯å¾„\n")
            f.write("="*100 + "\n")
        
        print(f"      Paths saved to: {filename}")
        return filename
    











