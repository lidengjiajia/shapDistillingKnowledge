# SHAP-guided Adaptive Knowledge Distillation for Credit Scoring
# SHAPå¼•å¯¼çš„è‡ªé€‚åº”çŸ¥è¯†è’¸é¦ä¿¡ç”¨è¯„åˆ†ç³»ç»Ÿ

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A comprehensive framework for interpretable credit scoring using SHAP-guided knowledge distillation with theoretical foundations.

## ğŸ¯ Key Features

- **Academic Baseline Models**: LR-Ridge, LR-Lasso, LR-ElasticNet, SVM-RBF, RF, GBDT, XGBoost, LightGBM, CatBoost
- **Neural Teacher Models**: MLP, ResNet, Transformer architectures
- **SAKD Framework**: SHAP-guided Adaptive Knowledge Distillation with theoretical proofs
- **SHAP Interpretability**: Feature importance, stability analysis, and visualizations
- **GPU Acceleration**: CUDA support for all deep learning and tree-based models
- **Systematic Ablation**: Temperature, alpha, and architecture ablation experiments

## ğŸ“Š Datasets

| Dataset | Samples | Features | Source |
|---------|---------|----------|--------|
| German Credit | 1,000 | 20 | UCI |
| Australian Credit | 690 | 14 | UCI |
| Xinwang Credit | 17,884 | 100 | Chinese P2P |
| UCI Credit Card | 30,000 | 23 | UCI |

## ğŸš€ Quick Start

### Installation

```bash
# Clone repository
git clone https://github.com/your-repo/credit-scoring-kd.git
cd credit-scoring-kd

# Install dependencies
pip install -r requirements.txt
```

### Requirements

```
torch>=2.0.0
scikit-learn>=1.0.0
xgboost>=1.7.0
lightgbm>=3.3.0
catboost>=1.0.0
shap>=0.41.0
pandas>=1.5.0
numpy>=1.21.0
matplotlib>=3.5.0
seaborn>=0.12.0
```

### Run Experiments

```bash
# Run full experiment pipeline
python run_experiments.py --dataset german --gpu

# Available datasets: german, australian, xinwang, uci
```

## ğŸ“ Project Structure

```
credit-scoring-kd/
â”œâ”€â”€ src/                          # Source code
â”‚   â”œâ”€â”€ data/                     # Data preprocessing
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ preprocessor.py       # DataPreprocessor class
â”‚   â”‚   â””â”€â”€ dataset.py            # PyTorch Dataset
â”‚   â”œâ”€â”€ models/                   # Model implementations
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ baselines.py          # Traditional ML baselines
â”‚   â”‚   â”œâ”€â”€ neural.py             # Neural network models
â”‚   â”‚   â””â”€â”€ sota_baselines.py     # SOTA models (TabNet, etc.)
â”‚   â”œâ”€â”€ distillation/             # Knowledge distillation
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ sakd_framework.py     # SAKD with theoretical proofs
â”‚   â”‚   â””â”€â”€ advanced_distillation.py
â”‚   â”œâ”€â”€ interpretability/         # SHAP analysis
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ shap_analyzer.py      # SHAPAnalyzer class
â”‚   â””â”€â”€ utils/                    # Utilities
â”‚       â”œâ”€â”€ config_manager.py
â”‚       â””â”€â”€ experiment_tracker.py
â”œâ”€â”€ config/                       # Configuration files
â”‚   â””â”€â”€ experiment_config.yaml
â”œâ”€â”€ data/                         # Datasets
â”‚   â”œâ”€â”€ german_credit.csv
â”‚   â”œâ”€â”€ australian_credit.csv
â”‚   â””â”€â”€ xinwang.csv
â”œâ”€â”€ results/                      # Experiment outputs
â”œâ”€â”€ visualization/                # Plotting utilities
â”‚   â””â”€â”€ ablation_plots.py
â”œâ”€â”€ run_experiments.py            # Main experiment runner
â””â”€â”€ README.md                     # This file
```

## ğŸ“ Theoretical Foundations

### Theorem 1: Temperature-Interpretability Tradeoff

$$\mathbb{E}[\|p_S - p_T\|_2] \leq \frac{C_1}{\sqrt{\tau}} + C_2 \cdot \exp\left(-\frac{\tau}{\tau_0}\right)$$

### Theorem 2: Generalization Bound for SHAP-guided Distillation

$$\epsilon_S \leq \epsilon_T + O\left(\sqrt{\frac{k \cdot \log k}{n}}\right) + O\left(d_{\max}^{-1}\right) + O\left(\frac{1}{\tau}\right)$$

### Theorem 3: Feature Selection Consistency

$$P\left(|S_k \cap S_k^*| \geq (1-\delta)k\right) \geq 1 - 2\exp\left(-\frac{n\delta^2}{2}\right)$$

## ğŸ”¬ Baseline Models

| Model | Category | Reference |
|-------|----------|-----------|
| LR-Ridge | Linear | Hosmer & Lemeshow (2000) |
| LR-Lasso | Linear | Tibshirani (1996) |
| LR-ElasticNet | Linear | Zou & Hastie (2005) |
| SVM-RBF | Kernel | Cortes & Vapnik (1995) |
| RF | Ensemble | Breiman (2001) |
| GBDT | Ensemble | Friedman (2001) |
| XGBoost | Ensemble | Chen & Guestrin (2016) |
| LightGBM | Ensemble | Ke et al. (2017) |
| CatBoost | Ensemble | Prokhorenkova et al. (2018) |

## ğŸ“ˆ Ablation Experiments

| Dimension | Values | Purpose |
|-----------|--------|---------|
| Temperature (Ï„) | {1, 2, 4, 8, 16} | Theorem 1 validation |
| Alpha (Î±) | {0.3, 0.5, 0.7, 0.9} | Soft/hard target balance |
| Architecture | Tiny/Small/Medium/Large | Model complexity analysis |

## ğŸ–¥ï¸ GPU Configuration

The framework automatically detects and uses GPU when available:

```python
# Automatic GPU detection
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# XGBoost GPU
xgb.XGBClassifier(tree_method='hist', device='cuda')

# LightGBM GPU
lgb.LGBMClassifier(device='gpu')

# CatBoost GPU
CatBoostClassifier(task_type='GPU')
```

## ğŸ“Š Example Results

### German Credit Dataset

| Model | AUC | Accuracy | F1 |
|-------|-----|----------|-----|
| LR-Ridge | 0.756 | 0.725 | 0.712 |
| XGBoost | 0.867 | 0.834 | 0.821 |
| CatBoost | 0.873 | 0.841 | 0.828 |
| **SAKD-Student** | **0.879** | **0.848** | **0.835** |

## ğŸ“ Citation

If you use this code in your research, please cite:

```bibtex
@article{author2024sakd,
  title={SHAP-guided Adaptive Knowledge Distillation for Interpretable Credit Scoring},
  author={Author, A. and Author, B.},
  journal={Financial Innovation},
  year={2024}
}
```

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

