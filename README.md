# ğŸ¯ åŸºäºSHAPå¼•å¯¼çš„çŸ¥è¯†è’¸é¦ä¿¡ç”¨è¯„åˆ†ç³»ç»Ÿ# SHAP-Guided Knowledge Distillation for Credit Scoring



## ğŸ“‹ é¡¹ç›®ç®€ä»‹## ğŸ¯ Project Overview



**SHAP-Guided Knowledge Distillation for Credit Scoring****åŸºäºSHAPç‰¹å¾é‡è¦æ€§å¼•å¯¼çš„çŸ¥è¯†è’¸é¦ä¿¡ç”¨è¯„åˆ†ç³»ç»Ÿ**



æœ¬é¡¹ç›®å®ç°äº†ä¸€ä¸ªåˆ›æ–°çš„ä¿¡ç”¨è¯„åˆ†ç³»ç»Ÿï¼Œå°†æ·±åº¦ç¥ç»ç½‘ç»œçš„é¢„æµ‹èƒ½åŠ›ä¸å†³ç­–æ ‘çš„å¯è§£é‡Šæ€§ç›¸ç»“åˆï¼Œé€šè¿‡çŸ¥è¯†è’¸é¦æŠ€æœ¯å®ç°æ™ºèƒ½ç‰¹å¾é€‰æ‹©å’Œæ¨¡å‹å‹ç¼©ã€‚ç³»ç»Ÿä½¿ç”¨SHAPï¼ˆSHapley Additive exPlanationsï¼‰æ–¹æ³•è¿›è¡Œç‰¹å¾é‡è¦æ€§åˆ†æï¼Œå¼•å¯¼çŸ¥è¯†è’¸é¦è¿‡ç¨‹ï¼Œåœ¨ä¿æŒé«˜å‡†ç¡®ç‡çš„åŒæ—¶å¤§å¹…æå‡æ¨¡å‹å¯è§£é‡Šæ€§ã€‚This project implements a comprehensive framework for **SHAP-guided knowledge distillation** in credit scoring applications. The system combines the interpretability of decision trees with the predictive power of deep neural networks through innovative knowledge distillation techniques, using SHAP (SHapley Additive exPlanations) for intelligent feature selection.



### âœ¨ æ ¸å¿ƒåˆ›æ–°ç‚¹---



1. **SHAPå¼•å¯¼çš„ç‰¹å¾é€‰æ‹©**: ä½¿ç”¨SHAPå€¼è¯†åˆ«æœ€é‡è¦çš„ç‰¹å¾ï¼Œå®ç°æ™ºèƒ½é™ç»´## ğŸ“ Project Structure

2. **çŸ¥è¯†è’¸é¦æŠ€æœ¯**: å°†å¤æ‚ç¥ç»ç½‘ç»œï¼ˆæ•™å¸ˆæ¨¡å‹ï¼‰çš„çŸ¥è¯†è¿ç§»åˆ°ç®€å•å†³ç­–æ ‘ï¼ˆå­¦ç”Ÿæ¨¡å‹ï¼‰

3. **å…¨é¢çš„æ¶ˆèå®éªŒ**: ç³»ç»Ÿåˆ†æTop-kç‰¹å¾æ•°é‡ã€æ¸©åº¦å‚æ•°ã€è’¸é¦æƒé‡ç­‰å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“```

4. **å¯è§£é‡Šå†³ç­–è§„åˆ™**: è‡ªåŠ¨æå–å¹¶å±•ç¤ºæ˜“äºç†è§£çš„å†³ç­–è§„åˆ™Financial innovation/

â”œâ”€â”€ data/                          # Dataset storage

---â”‚   â”œâ”€â”€ german_credit.csv         # German Credit Dataset (1,000 samples, 54 features)

â”‚   â”œâ”€â”€ australian_credit.csv     # Australian Credit Dataset (690 samples, 22 features)

## ğŸ“ é¡¹ç›®ç»“æ„â”‚   â””â”€â”€ uci_credit.xls           # UCI Taiwan Credit Dataset (30,000 samples, 23 features)

â”œâ”€â”€ results/                       # Generated output files

```â”‚   â”œâ”€â”€ model_comparison_*.xlsx    # Model performance comparison

Financial innovation/â”‚   â”œâ”€â”€ shap_feature_importance.png # SHAP feature visualization

â”œâ”€â”€ data/                              # æ•°æ®é›†ç›®å½•â”‚   â”œâ”€â”€ ablation_study_analysis_*.png # Ablation study plots

â”‚   â”œâ”€â”€ german_credit.csv             # å¾·å›½ä¿¡ç”¨æ•°æ®é›† (1,000æ ·æœ¬, 54ç‰¹å¾)â”‚   â”œâ”€â”€ topk_ablation_study_analysis_*.png # Top-k ablation analysis

â”‚   â”œâ”€â”€ australian_credit.csv         # æ¾³å¤§åˆ©äºšä¿¡ç”¨æ•°æ®é›† (690æ ·æœ¬, 22ç‰¹å¾)â”‚   â”œâ”€â”€ best_all_feature_rules_*.txt # Full feature decision rules

â”‚   â””â”€â”€ uci_credit.xls               # UCIå°æ¹¾ä¿¡ç”¨æ•°æ®é›† (30,000æ ·æœ¬, 23ç‰¹å¾)â”‚   â””â”€â”€ best_topk_rules_*.txt     # Top-k feature decision rules

â”œâ”€â”€ results/                           # å®éªŒç»“æœç›®å½•â”œâ”€â”€ main.py                       # Main execution pipeline

â”‚   â”œâ”€â”€ model_comparison_*.xlsx        # æ¨¡å‹æ€§èƒ½å¯¹æ¯”è¡¨â”œâ”€â”€ data_preprocessing.py         # Data loading and preprocessing

â”‚   â”œâ”€â”€ shap_*_features.png           # SHAPç‰¹å¾é‡è¦æ€§å¯è§†åŒ–â”œâ”€â”€ neural_models.py             # Neural network teacher models

â”‚   â”œâ”€â”€ topk_ablation_visualization_*.png  # Top-kæ¶ˆèå®éªŒåˆ†æå›¾â”œâ”€â”€ distillation_module.py       # Knowledge distillation core

â”‚   â”œâ”€â”€ depth_ablation_visualization_*.png # å†³ç­–æ ‘æ·±åº¦æ¶ˆèåˆ†æå›¾â”œâ”€â”€ shap_analysis.py             # SHAP feature importance analysis

â”‚   â”œâ”€â”€ best_all_feature_rules_*.txt   # å…¨ç‰¹å¾å†³ç­–è§„åˆ™â”œâ”€â”€ ablation_analyzer.py         # Ablation study visualization

â”‚   â””â”€â”€ best_topk_rules_*.txt         # Top-kç‰¹å¾å†³ç­–è§„åˆ™â”œâ”€â”€ result_manager.py            # Output management and reporting

â”œâ”€â”€ main.py                           # ä¸»ç¨‹åºå…¥å£â””â”€â”€ README.md                    # This documentation

â”œâ”€â”€ data_preprocessing.py             # æ•°æ®åŠ è½½å’Œé¢„å¤„ç†```

â”œâ”€â”€ neural_models.py                  # ç¥ç»ç½‘ç»œæ•™å¸ˆæ¨¡å‹

â”œâ”€â”€ distillation_module.py           # çŸ¥è¯†è’¸é¦æ ¸å¿ƒæ¨¡å—---

â”œâ”€â”€ shap_analysis.py                 # SHAPç‰¹å¾é‡è¦æ€§åˆ†æ

â”œâ”€â”€ ablation_analyzer.py             # æ¶ˆèå®éªŒå¯è§†åŒ–## ğŸ§  Teacher Model Architectures

â”œâ”€â”€ result_manager.py                # ç»“æœç®¡ç†å’ŒæŠ¥å‘Šç”Ÿæˆ

â”œâ”€â”€ clean_results.py                 # ç»“æœæ¸…ç†è„šæœ¬### German Credit Dataset (1,000 samples, 54 features)

â””â”€â”€ README.md                        # é¡¹ç›®æ–‡æ¡£**Enhanced Residual Neural Network** - ä¼˜åŒ–çš„æ®‹å·®ç½‘ç»œæ¶æ„

```- **Architecture**: Residual blocks with skip connections for improved gradient flow

- **Layers**:

---  - Input: Linear(54 â†’ 512) + BatchNorm + ReLU + Dropout(0.3)

  - Residual Block 1: [Linear(512 â†’ 256) â†’ BatchNorm â†’ ReLU â†’ Linear(256 â†’ 256) â†’ BatchNorm] + Skip(512 â†’ 256)

## ğŸ”¬ æŠ€æœ¯æ¶æ„  - Residual Block 2: [Linear(256 â†’ 128) â†’ BatchNorm â†’ ReLU â†’ Linear(128 â†’ 128) â†’ BatchNorm] + Skip(256 â†’ 128)

  - Output: Linear(128 â†’ 64) â†’ BatchNorm â†’ ReLU â†’ Linear(64 â†’ 32) â†’ ReLU â†’ Linear(32 â†’ 1)

### 1. æ•™å¸ˆæ¨¡å‹ï¼ˆTeacher Modelsï¼‰- **Loss Function**: BCEWithLogitsLoss with class balancing (pos_weight for imbalanced data)

- **Optimization**: AdamW (lr=0.0005, weight_decay=1e-3), ReduceLROnPlateau scheduler

#### Germanä¿¡ç”¨æ•°æ®é›†- **Training**: 100 epochs (optimized), patience=30, batch_size=32

- **æ¶æ„**: å¢å¼ºæ®‹å·®ç¥ç»ç½‘ç»œ (Enhanced Residual Network)- **Target Accuracy**: 75%+ (improved from previous 62%)

- **ç‰¹ç‚¹**: æ®‹å·®è¿æ¥ + æ‰¹å½’ä¸€åŒ– + Dropoutæ­£åˆ™åŒ–- **Reference**: Residual Networks (ResNet) - He et al. (2016)

- **å±‚ç»“æ„**:

  - è¾“å…¥å±‚: Linear(54 â†’ 512) + BatchNorm + ReLU + Dropout(0.3)### Australian Credit Dataset (690 samples, 22 features)  

  - æ®‹å·®å—1: [Linear(512 â†’ 256) â†’ BatchNorm â†’ ReLU â†’ Linear(256 â†’ 256) â†’ BatchNorm] + Skip(512 â†’ 256)**Deep Feed-Forward Network** - æ·±åº¦å‰é¦ˆç½‘ç»œ

  - æ®‹å·®å—2: [Linear(256 â†’ 128) â†’ BatchNorm â†’ ReLU â†’ Linear(128 â†’ 128) â†’ BatchNorm] + Skip(256 â†’ 128)- **Architecture**: Sequential layers with batch normalization and dropout

  - è¾“å‡ºå±‚: Linear(128 â†’ 64) â†’ BatchNorm â†’ ReLU â†’ Linear(64 â†’ 32) â†’ ReLU â†’ Linear(32 â†’ 1)- **Layers**: 

- **è®­ç»ƒå‚æ•°**:   - Linear(22 â†’ 256) â†’ BatchNorm â†’ ReLU â†’ Dropout(0.4)

  - ä¼˜åŒ–å™¨: AdamW (lr=0.0005, weight_decay=1e-3)  - Linear(256 â†’ 128) â†’ BatchNorm â†’ ReLU â†’ Dropout(0.35)

  - æŸå¤±å‡½æ•°: BCEWithLogitsLossï¼ˆç±»åˆ«åŠ æƒï¼‰  - Linear(128 â†’ 64) â†’ BatchNorm â†’ ReLU â†’ Dropout(0.3)

  - è®­ç»ƒè½®æ•°: 100 epochs, early_stopping patience=30  - Linear(64 â†’ 32) â†’ ReLU â†’ Dropout(0.25)

  - æ‰¹æ¬¡å¤§å°: 32  - Linear(32 â†’ 1) â†’ Sigmoid

- **Loss Function**: BCELoss (balanced dataset)

#### Australianä¿¡ç”¨æ•°æ®é›†- **Optimization**: AdamW (lr=0.002, weight_decay=1e-3), ReduceLROnPlateau scheduler  

- **æ¶æ„**: è½»é‡çº§ç¥ç»ç½‘ç»œ (Lightweight Network)- **Training**: 100 epochs (optimized), patience=20, batch_size=64

- **ç‰¹ç‚¹**: ç®€åŒ–ç»“æ„ï¼Œé€‚åº”å°æ ·æœ¬æ•°æ®- **Expected Accuracy**: 85%+

- **å±‚ç»“æ„**:- **Reference**: Deep Neural Networks for Credit Scoring - Khandani et al. (2010)

  - Linear(22 â†’ 128) + BatchNorm + ReLU + Dropout(0.3)

  - Linear(128 â†’ 64) + BatchNorm + ReLU + Dropout(0.2)### UCI Credit Default Dataset (30,000 samples, 23 features)

  - Linear(64 â†’ 32) + ReLU**Large-Scale Deep Network** - å¤§è§„æ¨¡æ·±åº¦ç½‘ç»œ

  - Linear(32 â†’ 1)- **Architecture**: Deep network optimized for large datasets

- **è®­ç»ƒå‚æ•°**: - **Layers**:

  - ä¼˜åŒ–å™¨: AdamW (lr=0.001, weight_decay=1e-4)  - Linear(23 â†’ 512) â†’ BatchNorm â†’ ReLU â†’ Dropout(0.5)

  - è®­ç»ƒè½®æ•°: 100 epochs, early_stopping patience=25  - Linear(512 â†’ 256) â†’ BatchNorm â†’ ReLU â†’ Dropout(0.45)

  - æ‰¹æ¬¡å¤§å°: 32  - Linear(256 â†’ 128) â†’ BatchNorm â†’ ReLU â†’ Dropout(0.4)

  - Linear(128 â†’ 64) â†’ BatchNorm â†’ ReLU â†’ Dropout(0.35)

#### UCIä¿¡ç”¨æ•°æ®é›†  - Linear(64 â†’ 32) â†’ ReLU â†’ Dropout(0.3)

- **æ¶æ„**: æ·±åº¦å…¨è¿æ¥ç½‘ç»œ (Deep Fully-Connected Network)  - Linear(32 â†’ 1) â†’ Sigmoid

- **ç‰¹ç‚¹**: å¤§å®¹é‡æ¨¡å‹ï¼Œé€‚åº”å¤§è§„æ¨¡æ•°æ®- **Loss Function**: BCELoss with focal loss characteristics for large-scale training

- **å±‚ç»“æ„**:- **Optimization**: AdamW (lr=0.001, weight_decay=1e-4), ReduceLROnPlateau scheduler

  - Linear(23 â†’ 256) + BatchNorm + ReLU + Dropout(0.3)- **Training**: 100 epochs (optimized), patience=25, batch_size=128  

  - Linear(256 â†’ 128) + BatchNorm + ReLU + Dropout(0.2)- **Expected Accuracy**: 82%+

  - Linear(128 â†’ 64) + BatchNorm + ReLU + Dropout(0.1)- **Reference**: Large-scale Credit Scoring - Lessmann et al. (2015)

  - Linear(64 â†’ 32) + ReLU

  - Linear(32 â†’ 1)---

- **è®­ç»ƒå‚æ•°**: 

  - ä¼˜åŒ–å™¨: AdamW (lr=0.001, weight_decay=1e-4)## ğŸ“Š Four-Model Comparison Framework

  - è®­ç»ƒè½®æ•°: 150 epochs, early_stopping patience=30

  - æ‰¹æ¬¡å¤§å°: 128æœ¬ç³»ç»Ÿè®­ç»ƒå¹¶å¯¹æ¯”ä»¥ä¸‹å››ç§æ¨¡å‹ï¼š



### 2. SHAPç‰¹å¾é‡è¦æ€§åˆ†æ### 1. Teacher Model (æ•™å¸ˆæ¨¡å‹)

- **æ¶æ„**: æ•°æ®é›†ç‰¹å®šçš„PyTorchæ·±åº¦ç¥ç»ç½‘ç»œ

**SHAP (SHapley Additive exPlanations)** æ˜¯ä¸€ç§åŸºäºåšå¼ˆè®ºçš„æ¨¡å‹è§£é‡Šæ–¹æ³•ï¼š- **ç‰¹ç‚¹**: é«˜é¢„æµ‹å‡†ç¡®æ€§ï¼Œå¤æ‚åº¦é«˜

- **ç›®çš„**: ä½œä¸ºçŸ¥è¯†è’¸é¦çš„æºæ¨¡å‹

- **æ ¸å¿ƒæ€æƒ³**: è®¡ç®—æ¯ä¸ªç‰¹å¾å¯¹é¢„æµ‹ç»“æœçš„è¾¹é™…è´¡çŒ®

- **ä¼˜åŠ¿**: ### 2. Baseline Decision Tree (åŸºå‡†å†³ç­–æ ‘)

  - ç†è®ºåŸºç¡€åšå®ï¼ˆShapleyå€¼çš„å”¯ä¸€æ€§ï¼‰- **æ¶æ„**: æ ‡å‡†scikit-learn DecisionTreeClassifier

  - æ¨¡å‹æ— å…³ï¼ˆå¯åº”ç”¨äºä»»ä½•æœºå™¨å­¦ä¹ æ¨¡å‹ï¼‰- **ç‰¹ç‚¹**: é«˜å¯è§£é‡Šæ€§ï¼Œç®€å•ç»“æ„

  - å±€éƒ¨å’Œå…¨å±€è§£é‡Šå…¼é¡¾- **ç›®çš„**: æä¾›åŸºå‡†æ€§èƒ½å¯¹æ¯”

- **å®ç°æµç¨‹**:

  1. ä½¿ç”¨TreeExplaineråˆ†ææ•™å¸ˆæ¨¡å‹### 3. All-Feature Distillation (å…¨ç‰¹å¾è’¸é¦)

  2. è®¡ç®—æ¯ä¸ªç‰¹å¾çš„å¹³å‡|SHAPå€¼|- **æ¶æ„**: ä½¿ç”¨å…¨éƒ¨ç‰¹å¾çš„çŸ¥è¯†è’¸é¦å†³ç­–æ ‘

  3. æŒ‰é‡è¦æ€§æ’åºï¼Œé€‰æ‹©Top-kç‰¹å¾- **ç‰¹ç‚¹**: å¹³è¡¡å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§

  4. ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š- **ç›®çš„**: å®Œæ•´ç‰¹å¾ç©ºé—´ä¸‹çš„çŸ¥è¯†è¿ç§»



### 3. çŸ¥è¯†è’¸é¦ï¼ˆKnowledge Distillationï¼‰### 4. Top-k Feature Distillation (Top-kç‰¹å¾è’¸é¦)

- **æ¶æ„**: åŸºäºSHAP Top-kç‰¹å¾çš„çŸ¥è¯†è’¸é¦å†³ç­–æ ‘

**æ ¸å¿ƒæœºåˆ¶**: å°†æ•™å¸ˆæ¨¡å‹çš„"è½¯çŸ¥è¯†"è¿ç§»åˆ°å­¦ç”Ÿæ¨¡å‹- **ç‰¹ç‚¹**: ç²¾ç®€ç‰¹å¾é›†ï¼Œé«˜æ•ˆè§£é‡Š

- **ç›®çš„**: æœ€ä¼˜ç‰¹å¾å­é›†ä¸‹çš„çŸ¥è¯†è¿ç§»

- **è½¯æ ‡ç­¾ç”Ÿæˆ**:

  ```---

  soft_labels = softmax(teacher_logits / T)

  ```## ğŸ”¬ Knowledge Distillation Process

  å…¶ä¸­Tä¸ºæ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶æ¦‚ç‡åˆ†å¸ƒçš„å¹³æ»‘ç¨‹åº¦

### æ ¸å¿ƒæŠ€æœ¯å‚æ•°

- **è’¸é¦æŸå¤±å‡½æ•°**:- **Temperature Scaling**: T âˆˆ {1, 2, 3, 4, 5} for soft label generation

  ```- **Loss Combination**: Î± âˆˆ {0.0, 0.1, ..., 1.0} for balancing hard and soft losses

  L_distill = Î± * L_hard + (1-Î±) * L_soft- **Dynamic Feature Selection**: 

  ```  - German Dataset: k âˆˆ {5, 6, 7, ..., 54}

  - L_hard: ç¡¬æ ‡ç­¾æŸå¤±ï¼ˆçœŸå®æ ‡ç­¾ï¼‰  - Australian Dataset: k âˆˆ {5, 6, 7, ..., 22}

  - L_soft: è½¯æ ‡ç­¾æŸå¤±ï¼ˆæ•™å¸ˆæ¨¡å‹è¾“å‡ºï¼‰  - UCI Dataset: k âˆˆ {5, 6, 7, ..., 23}

  - Î±: åŠ æƒç³»æ•°ï¼ˆ0åˆ°1ä¹‹é—´ï¼‰- **Tree Optimization**: Optuna-based hyperparameter tuning for decision trees

- **Decision Tree Depth**: max_depth âˆˆ {3, 4, 5, ..., 10}

- **å­¦ç”Ÿæ¨¡å‹**: å†³ç­–æ ‘ï¼ˆDecisionTreeClassifierï¼‰

  - å‚æ•°ä¼˜åŒ–: ä½¿ç”¨å›ºå®šå‚æ•°ç¡®ä¿å¯é‡å¤æ€§### è’¸é¦è¿‡ç¨‹

  - å›ºå®šå‚æ•°: max_depth(ç”±å®éªŒç¡®å®š), min_samples_split=2, min_samples_leaf=11. **Teacher Training**: è®­ç»ƒæ•°æ®é›†ç‰¹å®šçš„æ·±åº¦ç¥ç»ç½‘ç»œ

  - è’¸é¦å®ç°: ä½¿ç”¨è½¯æ ‡ç­¾æœ€å¤§æ¦‚ç‡ä½œä¸ºæ ·æœ¬æƒé‡2. **SHAP Analysis**: è®¡ç®—ç‰¹å¾é‡è¦æ€§å¹¶æ’åº

3. **Knowledge Transfer**: é€šè¿‡æ¸©åº¦ç¼©æ”¾è½¯æ ‡ç­¾è¿›è¡ŒçŸ¥è¯†è¿ç§»

### 4. æ¶ˆèå®éªŒï¼ˆAblation Studyï¼‰4. **Student Optimization**: åŸºäºæ··åˆæŸå¤±å‡½æ•°ä¼˜åŒ–å†³ç­–æ ‘å­¦ç”Ÿæ¨¡å‹

5. **Rule Extraction**: ä»è®­ç»ƒå¥½çš„å†³ç­–æ ‘ä¸­æå–å¯è§£é‡Šè§„åˆ™

ç³»ç»Ÿæ€§åˆ†æå„å‚æ•°å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼š

---

- **Top-kç‰¹å¾æ•°é‡**: k âˆˆ [5, n_features]  - Linear(256 â†’ 128) â†’ BatchNorm â†’ ReLU â†’ Dropout(0.4)

- **æ¸©åº¦å‚æ•°**: T âˆˆ {1, 2, 3, 4, 5}  - Linear(128 â†’ 64) â†’ BatchNorm â†’ ReLU â†’ Dropout(0.35)

- **è’¸é¦æƒé‡**: Î± âˆˆ {0.0, 0.1, 0.2, 0.3, 0.5, 0.7, 1.0}  - Linear(64 â†’ 32) â†’ ReLU â†’ Dropout(0.3)

- **å†³ç­–æ ‘æ·±åº¦**: max_depth âˆˆ {4, 5, 6, 7, 8}  - Linear(32 â†’ 1) â†’ Sigmoid

- **Loss Function**: BCELoss with focal loss characteristics for large-scale training

---- **Optimization**: AdamW (lr=0.001, weight_decay=1e-4), ReduceLROnPlateau scheduler

- **Training**: 300 epochs, patience=25, batch_size=128  

## ğŸš€ å¿«é€Ÿå¼€å§‹- **Expected Accuracy**: 82%+



### ç¯å¢ƒè¦æ±‚## ğŸ“Š Four-Model Comparison Framework



```bash1. **Teacher Model**: Dataset-specific deep neural networks (architectures above)

Python >= 3.82. **Baseline Decision Tree**: Standard scikit-learn DecisionTreeClassifier  

PyTorch >= 1.10.03. **All-Feature Distillation**: Knowledge distillation using complete feature set

scikit-learn >= 1.0.04. **Top-k Feature Distillation**: SHAP-guided feature selection for targeted distillation

shap >= 0.40.0

pandas >= 1.3.0## ğŸ”¬ Knowledge Distillation Process

numpy >= 1.21.0

matplotlib >= 3.4.0- **Temperature Scaling**: T âˆˆ {1, 2, 3, 4, 5} for soft label generation

seaborn >= 0.11.0- **Loss Combination**: Î± âˆˆ {0.0, 0.1, ..., 1.0} for balancing hard and soft losses

openpyxl >= 3.0.0- **Feature Selection**: Dynamic k ranges (German: 5-54, Australian: 5-22, UCI: 5-23)

optuna >= 3.0.0- **Tree Optimization**: Optuna-based hyperparameter tuning for decision trees

tqdm >= 4.62.0

```Financial innovation/



### å®‰è£…ä¾èµ–â”œâ”€â”€ data/                          # Dataset storage- **Knowledge Distillation**: å°†æ•™å¸ˆæ¨¡å‹çŸ¥è¯†è¿ç§»åˆ°å­¦ç”Ÿæ¨¡å‹



```bashâ”‚   â”œâ”€â”€ german_credit.csv          # German Credit Dataset

pip install torch scikit-learn shap pandas numpy matplotlib seaborn openpyxl optuna tqdm xlrd

```â”‚   â”œâ”€â”€ australian_credit.csv      # Australian Credit Dataset### ğŸ¯ Advanced Knowledge Distillation- **PyTorch Neural Networks**: é«˜æ€§èƒ½æ·±åº¦å­¦ä¹ æ•™å¸ˆæ¨¡å‹



### è¿è¡Œå®éªŒâ”‚   â””â”€â”€ uci_credit.xls            # UCI Taiwan Credit Dataset



```bashâ”œâ”€â”€ results/                       # Output files (generated)- **Temperature-scaled Soft Labels**: Configurable temperature parameter (T âˆˆ {1,2,3,4,5})- **Decision Tree**: å¯è§£é‡Šæ€§å¼ºçš„å­¦ç”Ÿæ¨¡å‹

# è¿è¡Œå®Œæ•´å®éªŒæµç¨‹

python main.pyâ”‚   â”œâ”€â”€ model_comparison_*.xlsx    # Performance comparison table

```

â”‚   â”œâ”€â”€ shap_feature_importance.png # SHAP visualization- **Hybrid Loss Function**: Balanced combination of hard and soft label losses (Î± âˆˆ {0.0,0.1,...,1.0})

å®éªŒæµç¨‹åŒ…æ‹¬ï¼š

1. æ•°æ®åŠ è½½å’Œé¢„å¤„ç†â”‚   â””â”€â”€ best_topk_rules_*.txt      # Extracted decision rules

2. æ•™å¸ˆæ¨¡å‹è®­ç»ƒï¼ˆç¥ç»ç½‘ç»œï¼‰

3. SHAPç‰¹å¾é‡è¦æ€§åˆ†æâ”œâ”€â”€ trained_models/               # Saved models (generated)- **Multi-depth Decision Trees**: Adaptive tree depth optimization (3-10 levels)---

4. çŸ¥è¯†è’¸é¦å’Œæ¨¡å‹è®­ç»ƒ

5. æ¶ˆèå®éªŒåˆ†æâ”‚   â”œâ”€â”€ teacher_model_*.pth       # PyTorch teacher models

6. ç»“æœå¯è§†åŒ–å’ŒæŠ¥å‘Šç”Ÿæˆ

â”‚   â”œâ”€â”€ teacher_model_*.pkl       # Scikit-learn format

### æ¸…ç†ç»“æœ

â”‚   â””â”€â”€ teacher_model_*.json      # Model metadata

```bash

# æ¸…ç©ºresultsæ–‡ä»¶å¤¹â”œâ”€â”€ main.py                       # Main execution pipeline### ğŸ“Š SHAP-Based Feature Selection  ## ç³»ç»Ÿæ¶æ„

python clean_results.py

```â”œâ”€â”€ data_preprocessing.py         # Data loading and preprocessing



---â”œâ”€â”€ neural_models.py             # Neural network architectures- **Intelligent Feature Ranking**: TreeExplainer-based SHAP value computation



## ğŸ“Š å®éªŒç»“æœâ”œâ”€â”€ distillation_module.py       # Knowledge distillation implementation



### æ€§èƒ½å¯¹æ¯”ç¤ºä¾‹â”œâ”€â”€ shap_analysis.py             # SHAP feature importance analysis- **Top-k Selection**: Systematic evaluation of k âˆˆ {5,6,7,8} most important features```



| æ•°æ®é›† | æ•™å¸ˆæ¨¡å‹å‡†ç¡®ç‡ | åŸºçº¿å†³ç­–æ ‘ | Top-kå†³ç­–æ ‘ | ç‰¹å¾å‹ç¼©ç‡ |â”œâ”€â”€ result_manager.py            # Output management and reporting

|--------|---------------|-----------|------------|----------|

| German | 76.5% | 74.2% | 76.0% (k=35) | 35.2% |â”œâ”€â”€ teacher_model_saver.py       # Model serialization utilities- **Cross-Dataset Analysis**: Comparative feature importance across datasetsâ”œâ”€â”€ data/                          # æ•°æ®é›†

| Australian | 87.0% | 85.5% | 87.2% (k=12) | 45.5% |

| UCI | 82.3% | 80.1% | 81.9% (k=20) | 13.0% |â””â”€â”€ README.md                    # This documentation



### å¯è§†åŒ–è¾“å‡º```â”‚   â”œâ”€â”€ uci_credit.xls            # UCIä¿¡ç”¨å¡æ•°æ®é›†



1. **SHAPç‰¹å¾é‡è¦æ€§å›¾**: 

   - `shap_german_features.png` - Germanæ•°æ®é›†ç‰¹å¾é‡è¦æ€§

   - `shap_australian_features.png` - Australianæ•°æ®é›†ç‰¹å¾é‡è¦æ€§## ï¿½ SHAP Feature Analysis

   - `shap_uci_features.png` - UCIæ•°æ®é›†ç‰¹å¾é‡è¦æ€§

### SHAPæ–¹æ³•ç‰¹ç‚¹

2. **æ¶ˆèå®éªŒåˆ†æå›¾**:- **TreeExplainer**: é’ˆå¯¹å†³ç­–æ ‘æ¨¡å‹ä¼˜åŒ–çš„SHAPè§£é‡Šå™¨

   - `topk_ablation_visualization_*.png` - Top-kç‰¹å¾æ•°é‡å½±å“åˆ†æ- **å…¨æ•°æ®é›†åˆ†æ**: ä½¿ç”¨è®­ç»ƒ+éªŒè¯+æµ‹è¯•çš„å®Œæ•´æ•°æ®é›†

   - `depth_ablation_visualization_*.png` - å†³ç­–æ ‘æ·±åº¦å½±å“åˆ†æ- **ç²¾ç¡®ç‰¹å¾æ’åº**: åŸºäºå¹³å‡ç»å¯¹SHAPå€¼è¿›è¡Œç‰¹å¾é‡è¦æ€§æ’å

- **å¯è§†åŒ–è¾“å‡º**: ç”ŸæˆTop-20ç‰¹å¾çš„å¯¹æ¯”å›¾è¡¨

3. **å†³ç­–è§„åˆ™æ–‡ä»¶**:

   - `best_topk_rules_*.txt` - æœ€ä¼˜Top-kæ¨¡å‹çš„å¯è§£é‡Šå†³ç­–è§„åˆ™### ç‰¹å¾é‡è¦æ€§å¯è§†åŒ–

   - `best_all_feature_rules_*.txt` - å…¨ç‰¹å¾æ¨¡å‹çš„å†³ç­–è§„åˆ™- **æ•°æ®é›†é¡ºåº**: German â†’ Australian â†’ UCI

- **é¢œè‰²æ–¹æ¡ˆ**: æµ…è“è‰²ç³» â†’ æµ…ç»¿è‰²ç³» â†’ æµ…æ©™è‰²ç³»

4. **æ€§èƒ½å¯¹æ¯”è¡¨**:- **ç‰¹å¾æ•°é‡**: æ¯ä¸ªæ•°æ®é›†æ˜¾ç¤ºTop-20é‡è¦ç‰¹å¾

   - `model_comparison_*.xlsx` - å„æ¨¡å‹è¯¦ç»†æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”- **çœŸå®ç‰¹å¾å**: ä½¿ç”¨è‹±æ–‡åŸå§‹ç‰¹å¾åè€Œéç¼–ç å



------



## ğŸ” ä»£ç æ¨¡å—è¯´æ˜## ğŸ”§ Core Modules



### main.py### 1. Data Preprocessing (`data_preprocessing.py`)

ä¸»ç¨‹åºå…¥å£ï¼Œåè°ƒæ•´ä¸ªå®éªŒæµç¨‹ï¼š- **åŠŸèƒ½**: åŠ è½½å’Œé¢„å¤„ç†ä¸‰ä¸ªä¿¡ç”¨æ•°æ®é›†

- æ•°æ®åŠ è½½å’Œåˆ’åˆ†- **æ ¸å¿ƒç‰¹æ€§**:

- æ•™å¸ˆæ¨¡å‹è®­ç»ƒ  - æ ‡å‡†åŒ–çš„æ•°æ®åŠ è½½å’Œtrain/validation/teståˆ’åˆ†

- SHAPåˆ†æ  - åˆ†ç±»å˜é‡çš„ç‰¹å¾ç¼–ç 

- çŸ¥è¯†è’¸é¦å®éªŒ  - æ•°æ®ç¼©æ”¾å’Œæ ‡å‡†åŒ–

- ç»“æœæ±‡æ€»å’Œå¯è§†åŒ–  - ç‰¹å¾åè¿½è¸ªä»¥ä¿è¯å¯è§£é‡Šæ€§



### data_preprocessing.py### 2. Neural Network Models (`neural_models.py`)

æ•°æ®é¢„å¤„ç†æ¨¡å—ï¼š- **åŠŸèƒ½**: å®šä¹‰å’Œè®­ç»ƒæ•™å¸ˆç¥ç»ç½‘ç»œ

- åŠ è½½ä¸‰ä¸ªä¿¡ç”¨æ•°æ®é›†- **æ¶æ„ç‰¹ç‚¹**:

- æ•°æ®æ¸…æ´—å’Œç‰¹å¾å·¥ç¨‹  - å¸¦æ®‹å·®è¿æ¥çš„é«˜çº§å‰é¦ˆç½‘ç»œ

- æ ‡å‡†åŒ–å¤„ç†  - æ‰¹é‡æ ‡å‡†åŒ–å’Œdropoutæ­£åˆ™åŒ–

- è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†åˆ’åˆ†  - è‡ªé€‚åº”å­¦ä¹ ç‡è°ƒåº¦

  - æ—©åœå’Œæ¨¡å‹æ£€æŸ¥ç‚¹

### neural_models.py

ç¥ç»ç½‘ç»œæ•™å¸ˆæ¨¡å‹ï¼š### 3. SHAP Analysis (`shap_analysis.py`)

- CreditNetç±»ï¼ˆæ®‹å·®ç½‘ç»œæ¶æ„ï¼‰- **åŠŸèƒ½**: ä½¿ç”¨SHAPè¿›è¡Œç‰¹å¾é‡è¦æ€§åˆ†æ

- ä¸‰ä¸ªæ•°æ®é›†çš„ä¸“ç”¨æ¨¡å‹é…ç½®- **å¤„ç†æµç¨‹**:

- è®­ç»ƒå’Œè¯„ä¼°åŠŸèƒ½  - ä¸ºæ¯ä¸ªæ•°æ®é›†è®­ç»ƒä¼˜åŒ–çš„å†³ç­–æ ‘

- Early stoppingå’Œå­¦ä¹ ç‡è°ƒåº¦  - ä½¿ç”¨TreeExplainerè®¡ç®—SHAPå€¼

  - ç”Ÿæˆtop-kç‰¹å¾æ’å

### shap_analysis.py  - åˆ›å»ºå¸¦æœ‰æ­£ç¡®ç‰¹å¾åçš„å¯è§†åŒ–

SHAPç‰¹å¾é‡è¦æ€§åˆ†æï¼š

- TreeExplaineré›†æˆ### 4. Knowledge Distillation (`distillation_module.py`)

- ç‰¹å¾é‡è¦æ€§è®¡ç®—å’Œæ’åº- **åŠŸèƒ½**: ä»æ•™å¸ˆæ¨¡å‹å‘å­¦ç”Ÿæ¨¡å‹è½¬ç§»çŸ¥è¯†

- Top-kç‰¹å¾é€‰æ‹©- **å®ç°ç»†èŠ‚**:

- å¯è§†åŒ–ç”Ÿæˆï¼ˆå•ç‹¬ä¿å­˜æ¯ä¸ªæ•°æ®é›†ï¼‰  - æ¸©åº¦ç¼©æ”¾çš„è½¯æ ‡ç­¾ç”Ÿæˆ

  - æ··åˆæŸå¤±å‡½æ•°(ç¡¬æ ‡ç­¾+è½¯æ ‡ç­¾)

### distillation_module.py  - åŸºäºSHAPçš„top-kç‰¹å¾é€‰æ‹©

çŸ¥è¯†è’¸é¦æ ¸å¿ƒæ¨¡å—ï¼š  - ä»è®­ç»ƒå¥½çš„æ ‘ä¸­æå–å†³ç­–è§„åˆ™

- è½¯æ ‡ç­¾æå–

- æ¸©åº¦ç¼©æ”¾### 5. Result Management (`result_manager.py`)

- å†³ç­–æ ‘è’¸é¦è®­ç»ƒ- **åŠŸèƒ½**: ç»„ç»‡å’Œå¯¼å‡ºç»“æœ

- Top-kå’Œå…¨ç‰¹å¾å®éªŒ- **è¾“å‡ºå†…å®¹**:

- å¤šè¿›ç¨‹å¹¶è¡Œä¼˜åŒ–  - åŸºäºExcelçš„æ€§èƒ½å¯¹æ¯”

  - å†³ç­–è§„åˆ™æ–‡æœ¬æ–‡ä»¶

### ablation_analyzer.py  - æ¨¡å‹æ€§èƒ½æŒ‡æ ‡

æ¶ˆèå®éªŒåˆ†æï¼š

- å®éªŒç»“æœè®°å½•### 6. Ablation Analysis (`ablation_analyzer.py`)

- Top-kå’Œæ·±åº¦æ¶ˆèå¯è§†åŒ–- **åŠŸèƒ½**: æ¶ˆèå®éªŒåˆ†æå’Œå¯è§†åŒ–

- æœ€ä¼˜é…ç½®è¯†åˆ«- **è¾“å‡ºå›¾è¡¨**:

- æŠ¥å‘Šç”Ÿæˆ  - Top-kç‰¹å¾æ•°é‡æ¶ˆèå®éªŒ

  - å†³ç­–æ ‘æ·±åº¦æ¶ˆèå®éªŒ

### result_manager.py  - 1Ã—2å¸ƒå±€çš„ç®€åŒ–å›¾è¡¨

ç»“æœç®¡ç†æ¨¡å—ï¼š

- å†³ç­–è§„åˆ™æå–---

- ExcelæŠ¥å‘Šç”Ÿæˆ

- æ€§èƒ½å¯¹æ¯”è¡¨## ğŸ“ˆ Datasets

- ç»“æœæ–‡ä»¶ç»„ç»‡

ç³»ç»Ÿåœ¨ä¸‰ä¸ªå¹¿æ³›ä½¿ç”¨çš„ä¿¡ç”¨è¯„åˆ†åŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼š

### clean_results.py

ç»“æœæ¸…ç†å·¥å…·ï¼š### 1. German Credit Dataset (1,000 samples, 54 features)

- å®‰å…¨åˆ é™¤resultsæ–‡ä»¶å¤¹å†…å®¹- **æ¥æº**: UCI Machine Learning Repository

- æ–‡ä»¶å ç”¨æ£€æµ‹- **ä»»åŠ¡**: äºŒåˆ†ç±»(å¥½/åä¿¡ç”¨é£é™©)

- æ‰¹é‡æ¸…ç†åŠŸèƒ½- **ç‰¹å¾**: äººå£ç»Ÿè®¡å­¦ã€è´¦æˆ·çŠ¶æ€ã€ä¿¡ç”¨å†å²



---### 2. Australian Credit Approval Dataset (690 samples, 22 features)

- **æ¥æº**: UCI Machine Learning Repository  

## ğŸ“ ç†è®ºèƒŒæ™¯- **ä»»åŠ¡**: äºŒåˆ†ç±»(æ‰¹å‡†/æ‹’ç»ä¿¡ç”¨)

- **ç‰¹å¾**: åŒ¿ååŒ–çš„ç”³è¯·äººå±æ€§

### çŸ¥è¯†è’¸é¦ï¼ˆKnowledge Distillationï¼‰

- **æå‡ºè€…**: Hinton et al., 2015### 3. Taiwan Credit Card Default Dataset (30,000 samples, 23 features)

- **æ ¸å¿ƒæ€æƒ³**: å¤æ‚æ¨¡å‹ï¼ˆæ•™å¸ˆï¼‰â†’ç®€å•æ¨¡å‹ï¼ˆå­¦ç”Ÿï¼‰çŸ¥è¯†è¿ç§»- **æ¥æº**: UCI Machine Learning Repository

- **å…³é”®æŠ€æœ¯**: è½¯æ ‡ç­¾ã€æ¸©åº¦ç¼©æ”¾ã€æŸå¤±å‡½æ•°è®¾è®¡- **ä»»åŠ¡**: äºŒåˆ†ç±»(è¿çº¦/éè¿çº¦)

- **ç‰¹å¾**: æ”¯ä»˜å†å²ã€è´¦å•é‡‘é¢ã€äººå£ç»Ÿè®¡æ•°æ®

### SHAPå€¼ï¼ˆShapley Additive Explanationsï¼‰

- **ç†è®ºåŸºç¡€**: åšå¼ˆè®ºä¸­çš„Shapleyå€¼---

- **ç‰¹æ€§**: å”¯ä¸€æ€§ã€ä¸€è‡´æ€§ã€å±€éƒ¨å‡†ç¡®æ€§

- **åº”ç”¨**: æ¨¡å‹è§£é‡Šã€ç‰¹å¾é€‰æ‹©ã€å¼‚å¸¸æ£€æµ‹## ğŸš€ Installation & Usage



### å†³ç­–æ ‘ï¼ˆDecision Treeï¼‰### Prerequisites

- **ä¼˜åŠ¿**: é«˜å¯è§£é‡Šæ€§ã€éçº¿æ€§å»ºæ¨¡ã€æ— éœ€ç‰¹å¾ç¼©æ”¾```bash

- **æŒ‘æˆ˜**: æ˜“è¿‡æ‹Ÿåˆã€ä¸ç¨³å®šæ€§pip install torch scikit-learn pandas numpy matplotlib seaborn shap openpyxl optuna tqdm

- **æœ¬é¡¹ç›®ä¼˜åŒ–**: çŸ¥è¯†è’¸é¦ã€å›ºå®šå‚æ•°ã€æ ·æœ¬åŠ æƒ```



---### Quick Start

```bash

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–æŠ€å·§# Clone the repository

git clone https://github.com/lidengjia1/shapGuided_KnowledgeDistilling.git

### 1. éšæœºç§å­æ§åˆ¶cd shapGuided_KnowledgeDistilling

æ‰€æœ‰éšæœºè¿‡ç¨‹ç»Ÿä¸€è®¾ç½®seed=42ï¼Œç¡®ä¿å®éªŒ100%å¯é‡å¤ï¼š

- NumPyéšæœºæ•°ç”Ÿæˆå™¨# Run the complete pipeline

- PyTorchéšæœºæ•°ç”Ÿæˆå™¨python main.py

- CUDAéšæœºæ•°ç”Ÿæˆå™¨```

- Pythonå†…ç½®randomæ¨¡å—

- ç¯å¢ƒå˜é‡PYTHONHASHSEED### Expected Outputs

- cuDNNç¡®å®šæ€§ç®—æ³•è¿è¡Œå®Œæˆåï¼Œå°†ç”Ÿæˆä»¥ä¸‹æ ¸å¿ƒæ–‡ä»¶ï¼š



### 2. å¹¶è¡Œè®¡ç®—1. **æ¨¡å‹æ€§èƒ½å¯¹æ¯”è¡¨** (`results/model_comparison_*.xlsx`)

- ä½¿ç”¨å¤šè¿›ç¨‹/å¤šçº¿ç¨‹åŠ é€Ÿæ¶ˆèå®éªŒ   - å››ç§æ¨¡å‹çš„æ€§èƒ½æŒ‡æ ‡

- è‡ªåŠ¨æ£€æµ‹CPUæ ¸å¿ƒæ•°å¹¶åˆç†åˆ†é…   - å‡†ç¡®ç‡ã€F1åˆ†æ•°ã€ç²¾ç¡®ç‡ã€å¬å›ç‡

- Windowså¹³å°ç‰¹æ®Šä¼˜åŒ–ï¼ˆspawnæ¨¡å¼ï¼‰   - æ¯ç§é…ç½®çš„æœ€ä½³è¶…å‚æ•°



### 3. å†…å­˜ä¼˜åŒ–2. **SHAPç‰¹å¾é‡è¦æ€§å›¾** (`results/shap_feature_importance.png`)

- æ‰¹æ¬¡å¤„ç†å¤§æ•°æ®é›†   - ä¸‰ä¸ªæ•°æ®é›†çš„å¯è§†åŒ–å¯¹æ¯”

- åŠæ—¶é‡Šæ”¾ä¸­é—´å˜é‡   - æ¯ä¸ªæ•°æ®é›†çš„Top-20é‡è¦ç‰¹å¾

- éäº¤äº’å¼matplotlibåç«¯   - è‹±æ–‡æ ‡ç­¾å’Œæ­£ç¡®çš„ç‰¹å¾å



### 4. æ•°å€¼ç¨³å®šæ€§3. **Top-kå†³ç­–è§„åˆ™** (`results/best_topk_rules_*.txt`)

- ä½¿ç”¨BCEWithLogitsLossé¿å…æ•°å€¼æº¢å‡º   - ä»æœ€ä½³æ¨¡å‹æå–çš„å†³ç­–æ ‘è§„åˆ™

- æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸   - ç‰¹å¾é‡è¦æ€§æ’å

- BatchNormalizationç¨³å®šè®­ç»ƒ   - æ¨¡å‹æ€§èƒ½è¯¦æƒ…



---4. **æ¶ˆèå®éªŒå›¾** (`results/*_ablation_study_analysis_*.png`)

   - Top-kç‰¹å¾æ•°é‡æ¶ˆèå®éªŒ

## ğŸ› å¸¸è§é—®é¢˜   - å†³ç­–æ ‘æ·±åº¦æ¶ˆèå®éªŒ



### Q1: ä¸ºä»€ä¹ˆæ¯æ¬¡è¿è¡Œç»“æœä¼šæœ‰å·®å¼‚ï¼Ÿ---

**A**: ç°å·²å®Œå…¨ä¿®å¤ã€‚é€šè¿‡è®¾ç½®å®Œæ•´çš„éšæœºç§å­ï¼ˆNumPyã€PyTorchã€CUDAã€randomæ¨¡å—ã€PYTHONHASHSEEDã€cuDNNï¼‰ï¼Œç¡®ä¿å®éªŒå®Œå…¨å¯é‡å¤ï¼ˆå·®å¼‚=0ï¼‰ã€‚

## ï¿½ Experimental Configuration

### Q2: å¦‚ä½•è°ƒæ•´Top-kç‰¹å¾æ•°é‡èŒƒå›´ï¼Ÿ

**A**: åœ¨`main.py`ä¸­ä¿®æ”¹k_rangeså‚æ•°ï¼š### å‚æ•°ç©ºé—´

```python- **Top-kç‰¹å¾æ•°**: 

k_ranges = {  - German Dataset: k âˆˆ {5, 6, ..., 54}

    'german': (5, 54),      # ä»5åˆ°54  - Australian Dataset: k âˆˆ {5, 6, ..., 22}

    'australian': (5, 22),  # ä»5åˆ°22  - UCI Dataset: k âˆˆ {5, 6, ..., 23}

    'uci': (5, 23)          # ä»5åˆ°23- **è’¸é¦æ¸©åº¦**: T âˆˆ {1, 2, 3, 4, 5}

}- **æŸå¤±æƒé‡**: Î± âˆˆ {0.0, 0.1, 0.2, ..., 1.0}

```- **æ ‘æ·±åº¦**: max_depth âˆˆ {3, 4, 5, 6, 7, 8, 9, 10}



### Q3: å¦‚ä½•ä¿®æ”¹æ¶ˆèå®éªŒå‚æ•°ï¼Ÿ### è¯„ä¼°æŒ‡æ ‡

**A**: åœ¨`main.py`ä¸­è°ƒæ•´ä»¥ä¸‹å‚æ•°ï¼š- **å‡†ç¡®ç‡ (Accuracy)**: æ­£ç¡®é¢„æµ‹çš„æ¯”ä¾‹

```python- **F1åˆ†æ•° (F1-Score)**: ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡

temperature_range = [1, 2, 3, 4, 5]  # æ¸©åº¦å‚æ•°- **ç²¾ç¡®ç‡ (Precision)**: æ­£é¢„æµ‹ä¸­çš„æ­£ç¡®æ¯”ä¾‹

alpha_range = [0.0, 0.1, 0.2, 0.3, 0.5, 0.7, 1.0]  # è’¸é¦æƒé‡- **å¬å›ç‡ (Recall)**: å®é™…æ­£ä¾‹ä¸­çš„é¢„æµ‹æ­£ç¡®æ¯”ä¾‹

max_depth_range = [4, 5, 6, 7, 8]  # å†³ç­–æ ‘æ·±åº¦

```### å¹¶å‘ä¼˜åŒ–

- **Windowså¹³å°**: ä½¿ç”¨min(4, cpu_count//2)ä¸ªå¹¶å‘è¿›ç¨‹

### Q4: å¦‚ä½•åŠ å¿«å®éªŒé€Ÿåº¦ï¼Ÿ- **Linux/Macå¹³å°**: ä½¿ç”¨min(cpu_count-1, cpu_count)ä¸ªå¹¶å‘è¿›ç¨‹

**A**: - **è¿›åº¦æ˜¾ç¤º**: é›†æˆtqdmè¿›åº¦æ¡ï¼Œå®æ—¶æ˜¾ç¤ºè®­ç»ƒè¿›åº¦

1. å‡å°‘æ¶ˆèå®éªŒçš„å‚æ•°ç»„åˆ

2. ä½¿ç”¨GPUåŠ é€Ÿç¥ç»ç½‘ç»œè®­ç»ƒ---

3. å¢åŠ å¹¶è¡Œè¿›ç¨‹æ•°ï¼ˆå¦‚æœCPUæ ¸å¿ƒå……è¶³ï¼‰

4. å‡å°‘æ•°æ®é›†è§„æ¨¡ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰



### Q5: å†…å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ- **v2.0**: Complete refactoring with SHAP-guided distillation# Run complete analysis pipeline- **Accuracy**: åˆ†ç±»å‡†ç¡®ç‡

**A**: 

1. å‡å°æ‰¹æ¬¡å¤§å°ï¼ˆbatch_sizeï¼‰- **v1.9**: Enhanced neural network architectures

2. å‡å°‘ç¥ç»ç½‘ç»œå±‚æ•°æˆ–éšè—å•å…ƒæ•°

3. åˆ†æ‰¹å¤„ç†æ¶ˆèå®éªŒ- **v1.8**: Improved feature name handling and visualizationpython main.py- **Precision**: ç²¾ç¡®ç‡

4. å…³é—­ä¸å¿…è¦çš„å¯è§†åŒ–

- **v1.7**: Added comprehensive result management

### Q6: æ¶ˆèå›¾æ ‡æ³¨é‡å æ€ä¹ˆåŠï¼Ÿ

**A**: å·²ä¿®å¤ã€‚ç³»ç»Ÿç°åœ¨æ ¹æ®kå€¼å’Œdepthå€¼æ™ºèƒ½è°ƒæ•´æ ‡æ³¨ä½ç½®ï¼Œé¿å…é‡å ã€‚- **v1.6**: Optimized knowledge distillation pipeline```- **Recall**: å¬å›ç‡



---



## ğŸ“ å®éªŒå¯é‡å¤æ€§---- **F1-Score**: F1åˆ†æ•°



æœ¬é¡¹ç›®éµå¾ªä¸¥æ ¼çš„å¯é‡å¤æ€§æ ‡å‡†ï¼š



âœ… **å®Œæ•´çš„éšæœºç§å­æ§åˆ¶**: æ‰€æœ‰éšæœºè¿‡ç¨‹å‡è®¾ç½®å›ºå®šç§å­  *This project represents cutting-edge research in explainable AI for financial applications, combining the power of deep learning with the interpretability requirements of financial decision-making.*This will generate three key outputs:- **AUC**: ROCæ›²çº¿ä¸‹é¢ç§¯

âœ… **è¯¦ç»†çš„å‚æ•°è®°å½•**: æ‰€æœ‰å®éªŒå‚æ•°è‡ªåŠ¨ä¿å­˜  

âœ… **ç‰ˆæœ¬åŒ–çš„ä¾èµ–**: æ˜ç¡®æŒ‡å®šåº“ç‰ˆæœ¬è¦æ±‚  

âœ… **æ ‡å‡†åŒ–çš„æ•°æ®å¤„ç†**: æ•°æ®é¢„å¤„ç†æµç¨‹å›ºå®š  

âœ… **è‡ªåŠ¨åŒ–çš„å®éªŒæµç¨‹**: ä¸€é”®è¿è¡Œå®Œæ•´å®éªŒ  1. **Model Comparison Table** (`results/model_comparison_*.xlsx`)---

âœ… **é˜²é‡å¤ç”Ÿæˆæœºåˆ¶**: é¿å…ç›¸åŒæ–‡ä»¶é‡å¤ç”Ÿæˆ  

   - Performance metrics for all four model types

---

   - Statistical significance tests## ç¯å¢ƒé…ç½®

## ğŸ”„ æ›´æ–°æ—¥å¿—

   - Hyperparameter configurations

### v2.0.0 (2025-01-09)

- âœ… ä¿®å¤éšæœºç§å­æ§åˆ¶ï¼Œå®ç°100%å¯é‡å¤æ€§### ä¾èµ–å®‰è£…

- âœ… ä¼˜åŒ–æ¶ˆèå›¾æ ‡æ³¨ä½ç½®ï¼Œæ™ºèƒ½é¿å…é‡å 

- âœ… åˆ é™¤å†—ä½™çš„å›¾åƒç”Ÿæˆä»£ç 2. **SHAP Feature Importance Visualization** (`results/shap_feature_importance.png`)```bash

- âœ… é‡æ„READMEä¸ºä¸­æ–‡ç‰ˆæœ¬

- âœ… æ·»åŠ æ–‡ä»¶å­˜åœ¨æ€§æ£€æŸ¥ï¼Œé˜²æ­¢é‡å¤ç”Ÿæˆ   - Top-8 features for each datasetpip install torch pandas scikit-learn xgboost shap matplotlib openpyxl numpy

- âœ… æ”¹è¿›SHAPå¯è§†åŒ–é…è‰²æ–¹æ¡ˆ

   - Comparative importance scores```

### v1.0.0

- åˆå§‹ç‰ˆæœ¬å‘å¸ƒ   - Cross-dataset feature analysis

- å®ç°åŸºç¡€çŸ¥è¯†è’¸é¦åŠŸèƒ½

- SHAPç‰¹å¾é€‰æ‹©### è¿è¡Œç³»ç»Ÿ

- æ¶ˆèå®éªŒåˆ†æ

3. **Top-k Decision Rules** (`results/best_topk_rules_*.txt`)```bash

---

   - Interpretable IF-THEN rules from best distilled modelspython main.py

## ğŸ¤ è´¡çŒ®æŒ‡å—

## ğŸ“ˆ Key Findings

æ¬¢è¿è´¡çŒ®ä»£ç ã€æŠ¥å‘Šé—®é¢˜æˆ–æå‡ºæ”¹è¿›å»ºè®®ï¼

### ä¸»è¦å®éªŒç»“æœ

### è´¡çŒ®æ–¹å¼- **Top-kç‰¹å¾è’¸é¦**è¾¾åˆ°ä¸å…¨ç‰¹å¾æ¨¡å‹ç›¸å½“çš„å‡†ç¡®ç‡

1. Forkæœ¬ä»“åº“- **SHAPå¼•å¯¼çš„ç‰¹å¾é€‰æ‹©**æ˜¾è‘—æå‡äº†æ¨¡å‹å¯è§£é‡Šæ€§  

2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)- **çŸ¥è¯†è’¸é¦**æœ‰æ•ˆç¼©å°äº†å‡†ç¡®ç‡ä¸å¯è§£é‡Šæ€§ä¹‹é—´çš„å·®è·

3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)- **æ¸©åº¦ç¼©æ”¾å’ŒæŸå¤±åŠ æƒ**æ˜¯æœ‰æ•ˆè’¸é¦çš„å…³é”®æŠ€æœ¯

4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)

5. å¼€å¯Pull Request### æ€§èƒ½åŸºå‡†æµ‹è¯•



---| Dataset | Teacher (DNN) | Baseline Tree | All-Feature Distill | Top-k Distill |

|---------|---------------|---------------|-------------------|---------------|

## ğŸ“„ è®¸å¯è¯| German | 0.75-0.78 | 0.70-0.73 | 0.73-0.76 | 0.74-0.77 |

| Australian | 0.85-0.88 | 0.82-0.85 | 0.84-0.87 | 0.85-0.88 |

æœ¬é¡¹ç›®é‡‡ç”¨MITè®¸å¯è¯ã€‚| UCI Taiwan | 0.80-0.83 | 0.76-0.79 | 0.78-0.81 | 0.79-0.82 |



---*æ³¨ï¼šèŒƒå›´åæ˜ ä¸åŒè¶…å‚æ•°é…ç½®ä¸‹çš„æ€§èƒ½å˜åŒ–*



## ğŸ“§ è”ç³»æ–¹å¼---



å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·é€šè¿‡GitHub Issuesè”ç³»ã€‚## ğŸ“š Technical References



---æœ¬é¡¹ç›®åŸºäºä»¥ä¸‹å‰æ²¿ç ”ç©¶æˆæœï¼š



## ğŸ™ è‡´è°¢### çŸ¥è¯†è’¸é¦ç›¸å…³

- **Neural Network Distillation**: Hinton et al. (2015) - æ¸©åº¦ç¼©æ”¾å’Œè½¯æ ‡ç­¾è®­ç»ƒ

æ„Ÿè°¢ä»¥ä¸‹å¼€æºé¡¹ç›®å’Œç ”ç©¶å·¥ä½œï¼š- **Tabular Data Distillation**: é’ˆå¯¹è¡¨æ ¼æ•°æ®çš„çŸ¥è¯†è’¸é¦ä¼˜åŒ–

- PyTorchå›¢é˜Ÿ

- SHAPåº“ä½œè€… (Scott Lundberg)### SHAPå¯è§£é‡ŠAI

- scikit-learnç¤¾åŒº- **SHAP Values**: Lundberg & Lee (2017) - TreeExplainerç²¾ç¡®ç‰¹å¾é‡è¦æ€§è®¡ç®—

- Knowledge Distillationç›¸å…³ç ”ç©¶è€…- **Feature Selection**: åŸºäºSHAPçš„æ™ºèƒ½ç‰¹å¾é€‰æ‹©ç­–ç•¥



---### ç¥ç»æ¶æ„è®¾è®¡

- **Residual Networks**: He et al. (2016) - æ®‹å·®è¿æ¥æ”¹å–„æ¢¯åº¦æµ

## ğŸ“š å‚è€ƒæ–‡çŒ®- **Credit Scoring DNNs**: é’ˆå¯¹ä¿¡ç”¨è¯„åˆ†ä¼˜åŒ–çš„ç¥ç»ç½‘ç»œæ¶æ„



1. Hinton, G., Vinyals, O., & Dean, J. (2015). Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531.### é‡‘èæœºå™¨å­¦ä¹ 

- **Financial ML**: Lopez de Prado (2018) - é‡‘èé£é™©è¯„ä¼°å’Œå¯è§£é‡Šå»ºæ¨¡

2. Lundberg, S. M., & Lee, S. I. (2017). A unified approach to interpreting model predictions. Advances in neural information processing systems, 30.- **Regulatory Compliance**: ç¬¦åˆé‡‘èç›‘ç®¡è¦æ±‚çš„å¯è§£é‡ŠAIæ–¹æ³•



3. Breiman, L. (2001). Random forests. Machine learning, 45(1), 5-32.---



4. Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning. MIT press.## ğŸ”„ Version History



---### v2.0.0 (Current) - Enhanced Performance

- âœ… ä¼˜åŒ–æ•™å¸ˆæ¨¡å‹æ¶æ„ï¼Œæå‡Germanæ•°æ®é›†å‡†ç¡®ç‡è‡³75%+

**æœ€åæ›´æ–°**: 2025å¹´1æœˆ9æ—¥  - âœ… å‡å°‘è®­ç»ƒepochsï¼Œæé«˜è®­ç»ƒæ•ˆç‡

**ç‰ˆæœ¬**: 2.0.0  - âœ… ç®€åŒ–æ¶ˆèå®éªŒå›¾è¡¨ä¸º1Ã—2å¸ƒå±€

**çŠ¶æ€**: âœ… ç”Ÿäº§å°±ç»ª- âœ… æ”¹è¿›SHAPå¯è§†åŒ–é…è‰²æ–¹æ¡ˆ

- âœ… ç¦ç”¨æ–‡ä»¶è‡ªåŠ¨æ¸…ç†åŠŸèƒ½
- âœ… å¢å¼ºWindowså¹³å°å¹¶å‘æ”¯æŒ

### v1.0.0 - Initial Release
- âœ… åŸºç¡€çŸ¥è¯†è’¸é¦æ¡†æ¶
- âœ… SHAPç‰¹å¾é‡è¦æ€§åˆ†æ
- âœ… ä¸‰æ•°æ®é›†æ”¯æŒ
- âœ… åŸºç¡€å¯è§†åŒ–åŠŸèƒ½

---

## ğŸ“§ Contact Information

**Primary Author**: Li Dengjia  
**Email**: lidengjia@hnu.edu.cn  
**Institution**: Hunan University  
**Research Focus**: Financial AI, Knowledge Distillation, Explainable Machine Learning

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ¤ Acknowledgments

- UCI Machine Learning Repository for providing the benchmark datasets
- SHAP library developers for interpretability tools
- PyTorch team for the deep learning framework  
- Research community for advances in knowledge distillation and explainable AI

---

## ğŸ“– Citation

If you use this work in your research, please cite:

```bibtex
@misc{li2025shap_distillation,
  title={SHAP-Guided Knowledge Distillation for Credit Scoring},
  author={Li, Dengjia and [Co-authors]},
  year={2025},
  institution={Hunan University},
  note={A comprehensive framework for interpretable credit scoring using SHAP-guided knowledge distillation}
}
```

---

*This project represents ongoing research in interpretable machine learning for financial applications. Contributions and collaborations are welcome.*

**Last Updated**: September 16, 2025  
**Version**: v2.0.0
